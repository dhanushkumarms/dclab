=========================================================

			    18 QUESTIONS : SKILL BUILDER AND CHALLENGE 

=========================================================
=========================================================================================

Question -1

Q1. Problem Statement Alex is working on a navigation system that finds the shortest path between two locations using the A* algorithm. Given a directed graph where nodes represent locations and edges represent paths with weights, along with heuristic values for each node, write a program to determine the optimal path from a starting node to a destination node. If a path exists, print the sequence of nodes in the order they should be visited; otherwise, print that no path exists.

=========================================================================================

import heapq , defaultdict

def from_to_cost_edges(edges_dummy):
    edges = defaultdict(list)
    for frm, to, cost in edges_dummy:
        edges[frm].append((to, cost))
    return edges

def tuple_to_heuritic(heuristic_dummy):
    heuristic = {}
    for node, h_cost in heuristic_dummy:
        heuristic[node] = h_cost
    return heuristic

def a_star(edges, heuristic, start, goal):
    priority_queue = []

    #Kept the f-cost in o index then only sorting will happen inside the heap easily
    heapq.heappush(priority_queue, (0 + heuristic[start], 0, start, []))  # (f-cost, g-cost, node, path)

    visited = set()

    while priority_queue:
        f_cost, g_cost, current, path = heapq.heappop(priority_queue)

        if current in visited:
            continue
        visited.add(current)

        path = path + [current]

        if current == goal:
            return path, g_cost

        for neighbor, cost in edges.get(current, []):
            if neighbor not in visited:
                new_g_cost = g_cost + cost
                new_f_cost = new_g_cost + heuristic.get(neighbor, float('inf'))
                heapq.heappush(priority_queue, (new_f_cost, new_g_cost, neighbor, path))

    return None, float('inf')  # No path found

if __name__ == "__main__":

    n = int(input())  # Number of nodes

    e = int(input())  # Number of edges

    edges_dummy = []
    for _ in range(e):
        frm, to, cost = map(int, input().split())
        edges_dummy.append((frm, to, cost))

    heuristic_dummy = []
    h = int(input())  # Number of heuristic values

    for _ in range(h):
        node, h_cost = map(int, input().split())
        heuristic_dummy.append((node, h_cost))

    start = int(input())  # Start node

    goal = int(input())  # Goal node

    edges = from_to_cost_edges(edges_dummy)
    heuristic = tuple_to_heuritic(heuristic_dummy)

    shortest_path, total_cost = a_star(edges, heuristic, start, goal)

    if shortest_path:
        print("Shortest Path:", shortest_path)
        print("Total Cost:", total_cost)
    else:
        print("No path found")

=========================================================================================

Question -2

Q2. Problem Statement Implement the A* search algorithm to determine the minimum path cost from a starting node to a target node in a directed weighted graph. Each node in the graph has a list of neighbors with corresponding edge costs, and each node also has an associated heuristic value estimating its cost to the goal. If a path exists, output the total path cost; otherwise, output "Path does not exist!".

=========================================================================================
import heapq
import collections

def from_to_cost_edges(edges_dummy):
    edges = collections.defaultdict(list)
    for frm, to, cost in edges_dummy:
        edges[frm].append((to, cost))
    return edges

def tuple_to_heuristic(heuristic_dummy):
    return {node: h_cost for node, h_cost in heuristic_dummy}

def a_star(edges, heuristic, start, goal):
    priority_queue = []
    heapq.heappush(priority_queue, (heuristic[start], 0, start, []))  # (f-cost, g-cost, current_node, path)

    visited = set()

    while priority_queue:
        f_cost, g_cost, current, path = heapq.heappop(priority_queue)

        if current in visited:
            continue

        path = path + [current]
        visited.add(current)

        if current == goal:
            return path, g_cost  # Return the path and total cost

        for neighbor, edge_cost in edges[current]:
            if neighbor not in visited:
                new_g_cost = g_cost + edge_cost
                new_f_cost = new_g_cost + heuristic.get(neighbor, float('inf'))  # f = g + h
                heapq.heappush(priority_queue, (new_f_cost, new_g_cost, neighbor, path))

    return None, None  # No valid path

if __name__ == "__main__":
    n = int(input())  # Number of nodes
    e = int(input())  # Number of edges

    edges_dummy = []
    for _ in range(e):
        frm, to, cost = map(int, input().split())
        edges_dummy.append((frm, to, cost))

    h = int(input())  # Number of heuristic values
    heuristic_dummy = []
    for _ in range(h):
        node, h_cost = map(int, input().split())
        heuristic_dummy.append((node, h_cost))

    start = int(input())  # Start node
    goal = int(input())   # Goal node

    edges = from_to_cost_edges(edges_dummy)
    heuristic = tuple_to_heuristic(heuristic_dummy)

    shortest_path, total_cost = a_star(edges, heuristic, start, goal)

    if shortest_path:
        print("Path:", " -> ".join(map(str, shortest_path)))
        print("Total Cost:", total_cost)
    else:
        print("Path does not exist!")
=========================================================================================

Q3. Problem Statement George, a financial analyst, is working on optimizing the distribution of funds across various investment portfolios. Each investment scenario is represented as a node in a binary decision tree, where each node decides whether to increase or decrease investment in a particular portfolio.

George aims to use a minimax algorithm with alpha-beta pruning to determine the most favorable investment strategy, maximizing returns at various decision points.

=========================================================================================
#Here the premax and premin logic is used to correctly use the printStatements to pass testCase

def minimax(depth_level , depth, node_index, is_maximizing_player, values, alpha, beta):
    # Base case: If the depth is 0, return the value of the leaf node
    if depth == depth_level:
        return values[node_index]

    # If we are at a maximizing player
    if is_maximizing_player:
        max_val = float('-inf')
        pre_max = 0
        for i in range(2):  # Two children of each node
            value = minimax(depth_level + 1,depth ,  node_index * 2 + i , False, values, alpha, beta)
            max_val = max(max_val, value)
            # Update alpha
            if pre_max == 0 or pre_max != max_val:
                print(f"Maximizer updated best value to {max_val} at depth {depth_level}")

            alpha = max(alpha, max_val)

            if beta <= alpha:
                print(f"Pruning at depth {depth_level} by Maximizer")
                break
            pre_max = max_val

        return max_val

    # If we are at a minimizing player
    else:
        min_val = float('inf')
        pre_min = 0
        for i in range(2):  # Two children of each node
            value = minimax(depth_level + 1,depth , node_index * 2 + i , True, values, alpha, beta)
            min_val = min(min_val, value)
            # Update beta
            if pre_min == 0 or pre_min != min_val:
                print(f"Minimizer updated best value to {min_val} at depth {depth_level}")

            beta = min(beta, min_val)
            if beta <= alpha:
                print(f"Pruning at depth {depth_level} by Minimizer")
                break
            pre_min = min_val
        return min_val

def decision_tree(d, leaf_values):
    # Start from depth 0 (root) with alpha as negative infinity and beta as positive infinity
    optimal_value = minimax(0 , d , 0, True, leaf_values, float('-inf'), float('inf'))
    return optimal_value

# Input
d = int(input())  # depth of the tree

leaf_values = list(map(int, input().split()))  # leaf values at the bottom level of the tree

# Note if depth is not given Use this logic to get depth
# d = len(leaf_values).bit_length() - 1

# Output the result
optimal_value = decision_tree(d, leaf_values)
print(f"Optimal Value: {optimal_value}")

=========================================================================================

Q4. Problem Statement Julia, an environmental scientist, is working on optimizing water distribution strategies during periods of scarcity. She is using a decision-making tool that evaluates various strategies for allocating water to different regions based on predicted need and availability.

To improve the strategy, Julia implements a minimax algorithm with alpha-beta pruning within a decision tree, where each node represents a decision on water distribution. Additionally, Julia introduces a flat increase of 10 units of water at each node as a safety buffer, ensuring each area has an emergency reserve.

=========================================================================================

# I think , this Question similar like Question number 1(Refer My Machine Learning LAB Colab) in Alpha Beta Pruning that we done in examly portal.

import math

def alpha_beta_pruning(node, depth, alpha, beta, maximizing_player, water_supply):
    if depth == 0:
        return water_supply[node] + 10  # Adding 10 units as an emergency reserve

    if maximizing_player:
        max_eval = float('-inf')
        for i in range(2):  # Binary tree (each node has 2 children)
            value = alpha_beta_pruning(2 * node + i, depth - 1, alpha, beta, False, water_supply)
            max_eval = max(max_eval, value)
            alpha = max(alpha, max_eval)
            if beta <= alpha:
                break  # Beta cut-off
        return max_eval

    else:
        min_eval = float('inf')
        for i in range(2):
            value = alpha_beta_pruning(2 * node + i, depth - 1, alpha, beta, True, water_supply)
            min_eval = min(min_eval, value)
            beta = min(beta, min_eval)
            if beta <= alpha:
                break  # Alpha cut-off
        return min_eval

# Input: List of 8 integers representing available water at the leaf nodes
water_supply = list(map(int, input("Enter 8 water supply values: ").split()))
depth = len(water_supply).bit_length() - 1  # Depth of the tree

# Get the optimal water allocation
optimal_allocation = alpha_beta_pruning(0, depth, float('-inf'), float('inf'), True, water_supply)

# Output the results
print(f"Optimal water allocation: {optimal_allocation}")
# print(f"After 10% increase: {int(optimal_allocation * 1.10)}") Not Necessary

=========================================================================================

Question -5

Q5. Problem Statement Vino is a data analyst working with a loan dataset that contains information about urban and rural areas. The dataset has missing values that need to be addressed. Vino is also tasked with filling in missing values for the "Employment," "Population," and "Income" columns. Additionally, Vino needs to perform data standardization and normalization for the "Income" column. Handling Missing Data and Standardization: • Load the CSV dataset, and drop rows with missing values. • Fill categorical "Employment" with mode, "Population" with mean, and "Income" with median. • Display filled values. Standardize "Income_filled_median" using StandardScaler and normalize using MinMaxScaler.

=========================================================================================
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import os
import sys

# Load dataset
file_path = os.path.join(sys.path[0] , "urban_rural.csv")
df = pd.read_csv(file_path)

# Display missing data information
missing_data = df.isnull().sum()
print("Missing Data:")
print(missing_data)
print()

# Drop rows with missing values
df_dropna = df.dropna()
print("Dataset after dropping missing values:")
print(df_dropna.to_string(index=True))
print()

# Fill categorical column 'Employment' with mode
employment_imputer = SimpleImputer(strategy='most_frequent')
df['Employment_filled'] = employment_imputer.fit_transform(df[['Employment']])
print("Filled values for Employment with mode:")
print(df['Employment_filled'])
print()

# Fill numerical column 'Population' with mean
population_imputer = SimpleImputer(strategy='mean')
df['Population_filled_mean'] = population_imputer.fit_transform(df[['Population']])
print("Filled values for Population with mean:")
print(df['Population_filled_mean'])
print()

# Fill numerical column 'Income' with median
income_imputer = SimpleImputer(strategy='median')
df['Income_filled_median'] = income_imputer.fit_transform(df[['Income']])
print("Filled values for Income with median:")
print(df['Income_filled_median'])
print()

# Standardize 'Income_filled_median'
scaler = StandardScaler()
df['Income_Standardized'] = scaler.fit_transform(df[['Income_filled_median']])
print("Standardized Income:")
print(df[['Income_Standardized']])
print()

# Normalize 'Income_filled_median'
minmax_scaler = MinMaxScaler()
df['Income_Normalized'] = minmax_scaler.fit_transform(df[['Income_filled_median']])
print("Normalized Income:")
print(df[['Income_Normalized']])


#Without SimpleImputer

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import os
import sys

# Load dataset
file_path = os.path.join(sys.path[0], "urban_rural.csv")
df = pd.read_csv(file_path)

# Display missing data information
missing_data = df.isnull().sum()
print("Missing Data:")
print(missing_data)
print()

# Drop rows with missing values
df_dropna = df.dropna()
print("Dataset after dropping missing values:")
print(df_dropna.to_string(index=True))
print()

# Fill categorical column 'Employment' with mode
df['Employment_filled'] = df['Employment'].fillna(df['Employment'].mode()[0])
print("Filled values for Employment with mode:")
print(df['Employment_filled'])
print()

# Fill numerical column 'Population' with mean
df['Population_filled_mean'] = df['Population'].fillna(df['Population'].mean())
print("Filled values for Population with mean:")
print(df['Population_filled_mean'])
print()

# Fill numerical column 'Income' with median
df['Income_filled_median'] = df['Income'].fillna(df['Income'].median())
print("Filled values for Income with median:")
print(df['Income_filled_median'])
print()

# Standardize 'Income_filled_median'
scaler = StandardScaler()
df['Income_Standardized'] = scaler.fit_transform(df[['Income_filled_median']])
print("Standardized Income:")
print(df[['Income_Standardized']])
print()

# Normalize 'Income_filled_median'
minmax_scaler = MinMaxScaler()
df['Income_Normalized'] = minmax_scaler.fit_transform(df[['Income_filled_median']])
print("Normalized Income:")
print(df[['Income_Normalized']])

=========================================================================================

Question -6

Q6. Problem Statement Lora, a data analyst, is working on a project involving linear regression analysis. She has a dataset stored in a CSV file, consisting of two columns: 'x' and 'y'. Lora needs to perform linear regression analysis on this dataset to understand the relationship between 'x' and 'y'. To facilitate this analysis, you are tasked with developing a Python program. The program will prompt the user to input the filename of the dataset. It will then read the data from the CSV file, execute linear regression analysis using the Scipy library, and calculate the slope, intercept, and estimated value at x=10. Finally, it will print these results rounded to four decimal places.

=========================================================================================

#As in Question Mentioned I do linearReg with scipy.stats

import pandas as pd
import numpy as np
from scipy.stats import linregress
import os
import sys

# Function to load dataset
def load_dataset(filename):
    return pd.read_csv(filename)

# Function to perform linear regression
def perform_linear_regression(df):
    x = df['x']
    y = df['y']

    # Compute linear regression
    slope, intercept, r_value, p_value, std_err = linregress(x, y)

    # Estimate y when x = 10
    estimated_y = slope * 10 + intercept

    # Print results rounded to 4 decimal places
    print(f"Slope: {slope:.4f}")
    print(f"Intercept: {intercept:.4f}")
    print(f"Estimated value at x=10: {estimated_y:.4f}")

# Main function
def main():
    filename = input().strip()  # Take filename input
    df = load_dataset(os.path.join(sys.path[0], filename))
    perform_linear_regression(df)

if __name__ == "__main__":
    main()

#OtherWay

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import os
import sys

# Function to load the dataset
def load_dataset(filename):
    return pd.read_csv(filename)

# Function to train and evaluate the linear regression model
def train_and_evaluate_model(df):
    # Selecting features and target variable
    X = df[['x']]
    y = df['y']

    # Creating and training the model
    model = LinearRegression()
    model.fit(X, y)

    print(f"Slope: {model.coef_[0]:.4f}")
    print(f"Intercept: {model.intercept_:.4f}")

    # Predicting the value for x = 10
    estimated_y = model.predict(np.array([[10]]))[0]
    print(f"Estimated value at x=10: {estimated_y:.4f}")

# Main function to execute the program
def main():
    filename = input().strip()  # Taking filename input
    df = load_dataset(os.path.join(sys.path[0], filename))
    train_and_evaluate_model(df)

if __name__ == "__main__":
    main()

=========================================================================================

Question -7

Q7. Problem Statement Paul, a data scientist, is working on a project that involves evaluating the performance of a simple linear regression algorithm. He has a dataset stored in a CSV file, comprising two columns: 'x' and 'y'. Paul aims to calculate the root mean squared error (RMSE) of the regression algorithm applied to this dataset. To assist Paul, you are tasked with creating a Python program. The program will prompt the user to input the filename of the dataset. Subsequently, it will read the data from the CSV file, compute the RMSE using a simple linear regression algorithm, and output the RMSE value rounded to three decimal places.

=========================================================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import os
import sys

# Function to load the dataset
def load_dataset(filename):
    return pd.read_csv(filename)

# Function to train and evaluate the linear regression model
def train_and_evaluate_model(df):
    # Selecting features and target variable
    X = df[['x']]
    y = df['y']

    # Splitting data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Creating and training the model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Making predictions
    y_pred = model.predict(X_test)

    # Calculating RMSE
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    # Printing RMSE rounded to three decimal places
    print(f"Root Mean Squared Error (RMSE): {rmse:.3f}")

# Main function to execute the program
def main():
    filename = input().strip()  # Taking filename input
    df = load_dataset(os.path.join(sys.path[0], filename))
    train_and_evaluate_model(df)

if __name__ == "__main__":
    main()

=========================================================================================

Question -8

Q8. Problem Statement A data scientist is working on a customer churn prediction model for a subscription-based service. The dataset contains various attributes of customers, such as age, sex, account length, number of products used, credit card status, activity level, and estimated salary. The target variable Churn indicates whether a customer has canceled their subscription (1) or not (0). Write a program that reads the dataset from a CSV file, fits a logistic regression model to predict whether a customer will churn based on the given attributes, and evaluates the model's performance using accuracy, precision, recall, F1 score, AUC-ROC score, and confusion matrix. The program should output the evaluation metrics rounded to two decimal places.

=========================================================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import os
import sys

def load_dataset(filename):
    """Loads the dataset from a CSV file."""
    return pd.read_csv(filename)

def preprocess_data(df):
    """Prepares the dataset for training."""
    X = df.drop(columns=['Churn'])  # Features
    y = df['Churn']  # Target variable

    # Standardizing numerical features (optional but helps logistic regression)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return X_scaled, y

def train_and_evaluate_model(X, y):
    """Trains a Logistic Regression model and evaluates it."""
    # Splitting data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Creating and training the model
    model = LogisticRegression()
    model.fit(X_train, y_train)

    # Making predictions
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for AUC-ROC

    # Calculating evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_prob)
    cm = confusion_matrix(y_test, y_pred)

    # Printing evaluation metrics
    print("Logistic Regression Model Evaluation:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")
    print(f"AUC-ROC Score: {auc_roc:.2f}")
    print("Confusion Matrix:")
    print(cm)

def main():
    filename = input("Enter the dataset filename: ").strip()
    df = load_dataset(os.path.join(sys.path[0], filename))
    X, y = preprocess_data(df)
    train_and_evaluate_model(X, y)

if __name__ == "__main__":
    main()
=========================================================================================

Question -9

Q9. Problem Statement David, an HR manager, is analyzing employee performance using a dataset that contains hours_worked and a binary target promotion (1 if promoted, 0 otherwise). He needs to predict whether an employee will be promoted based on the number of hours they worked. Write a program that loads the dataset from a CSV file, applies logistic regression using hours_worked as the feature, and calculates the model's precision, recall, F1 score, accuracy, and confusion matrix. The program should output the precision score rounded to four decimal places, along with other metrics.

=========================================================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import os
import sys

# Function to load the dataset
def load_dataset(filename):
    return pd.read_csv(filename)

# Function to train and evaluate the logistic regression model
def train_and_evaluate_model(df):
    # Selecting feature and target variable
    X = df[['hours_worked']]
    y = df['promotion'] # Ref to csv file for correct target name

    # Splitting data into training and testing sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Creating and training the Logistic Regression model
    model = LogisticRegression()
    model.fit(X_train, y_train)

    # Making predictions
    y_pred = model.predict(X_test)

    # Evaluating the model
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)

    # Printing evaluation metrics
    print("Logistic Regression Model Evaluation:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("Confusion Matrix:")
    print(conf_matrix)

# Main function to execute the program
def main():
    filename = input().strip()  # Taking filename input
    df = load_dataset(os.path.join(sys.path[0], filename))
    train_and_evaluate_model(df)

if __name__ == "__main__":
    main()
=========================================================================================

Question -10

Q10. Problem Statement Emily is working on a machine learning project where she needs to classify passenger survival based on Titanic dataset features. Write a program to load a CSV file containing passenger details, preprocess the data by encoding categorical values and handling missing values, train an SVM classifier, and evaluate its performance using accuracy, precision, recall, F1-score, and a confusion matrix. The program should read the CSV filename from user input and output the models evaluation metrics.

=========================================================================================

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Prompt user for the filename
file_name = input("Enter the Titanic dataset filename: ").strip()

# Get the current directory of the script
current_directory = os.path.dirname(__file__)

# Construct the full path to the CSV file
file_path = os.path.join(current_directory, file_name)

# Load the dataset
df = pd.read_csv(file_path)

# Preprocess the dataset
def preprocess_data(df):
    # Selecting relevant columns
    df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]

    # Handle missing values
    df['Age'].fillna(df['Age'].median(), inplace=True)
    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
    df['Fare'].fillna(df['Fare'].median(), inplace=True)

    # Encode categorical variables
    label_encoder = LabelEncoder()
    df['Sex'] = label_encoder.fit_transform(df['Sex'])  # Male=1, Female=0
    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])  # S=2, C=0, Q=1

    # Split features (X) and target (y)
    X = df.drop(columns=['Survived'])
    y = df['Survived']

    # Split into training and test sets (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Feature scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test

# Train and evaluate the SVM model
def train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the SVM model
    svm_model = SVC(kernel='linear', random_state=42)
    svm_model.fit(X_train_scaled, y_train)

    # Predict on test data
    y_pred = svm_model.predict(X_test_scaled)

    # Evaluate model
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)

    # Print results
    print("\nSVM Model Evaluation:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print("\nClassification Report:")
    print(class_report)

# Main function to run the program
def main():
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)
    train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()
=========================================================================================

Question -11

Q11. Problem Statement Arjun, a data analyst, wants to automate the process of evaluating a loan approval model using an SVM classifier. Write a program to load a dataset from a given filename, train an SVM classifier with a polynomial kernel, and compute evaluation metrics such as accuracy, precision, recall, and F1-score. The program should handle cases where the dataset contains only one class and exit gracefully with an error message.

=========================================================================================

import os
import pandas as pd
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Prompt the user to enter the filename
file_name = input("Enter the dataset filename: ").strip()

# Get the current directory and construct the full path
current_directory = os.path.dirname(__file__)
file_path = os.path.join(current_directory, file_name)

# Load the dataset
try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    sys.exit(1)

# Display the first few rows (optional)
# print(df.head())

# Preprocess the dataset
def preprocess_data(df):
    # Handling categorical features if any
    label_encoder = LabelEncoder()

    # Encoding categorical columns (if present)
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = label_encoder.fit_transform(df[col])

    # Splitting features (X) and target variable (y)
    X = df.drop(columns=['Loan_Status'])  # Assuming 'Loan_Status' is the target column
    y = df['Loan_Status']

    # Checking if dataset contains only one class
    if len(y.unique()) == 1:
        print("Error: Dataset contains only one class. Classification cannot be performed.")
        sys.exit(1)

    # Splitting the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Feature scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test

# Train and evaluate the SVM model
def train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the SVM classifier with a polynomial kernel
    svm_model = SVC(kernel='poly', degree=3, random_state=42)
    svm_model.fit(X_train_scaled, y_train)

    # Predict on the test data
    y_pred = svm_model.predict(X_test_scaled)

    # Compute evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=1)
    recall = recall_score(y_test, y_pred, zero_division=1)
    f1 = f1_score(y_test, y_pred, zero_division=1)

    # Output the results
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

# Main function
def main():
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)
    train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()
=========================================================================================

Question -12

Q12. Problem Statement Emma, a retail analyst, needs to predict whether a product will be sold out based on historical sales data. Write a program that loads a dataset from a user-provided CSV file, preprocesses it by handling missing values, encoding categorical features, and scaling numerical features. Then, train a Gaussian Naïve Bayes model to classify whether a product is sold out and evaluate its performance using accuracy, confusion matrix, and classification report.

=========================================================================================

import os
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Prompt the user to enter the filename
file_name = input().strip()

# Construct the full path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Display first few rows (optional)
# print(df.head())

# Preprocess the dataset
def preprocess_data(df):
    # Handling missing values: Fill numerical values with the mean
    for col in df.select_dtypes(include=['number']).columns:
        df[col] = df[col].fillna(df[col].mean())

    # Fill missing categorical values with the most frequent value
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].fillna(df[col].mode()[0])

    # Encoding categorical features
    label_encoder = LabelEncoder()
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = label_encoder.fit_transform(df[col])

    # Splitting features (X) and target variable (y)
    X = df.drop(columns=['Sold_Out'])  # Assuming 'Sold_Out' is the target column
    y = df['Sold_Out']

    scalar = StandardScalar()
    X_scaled = scalar.fit_transform(X)

    # Split data into training and testing sets (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    return X_train, X_test, y_train, y_test

# Train and evaluate the Naïve Bayes model
def train_and_evaluate_nb(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the Gaussian Naïve Bayes model
    model = GaussianNB()
    model.fit(X_train_scaled, y_train)

    # Predict on test set
    y_pred = model.predict(X_test_scaled)

    # Compute accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Compute confusion matrix
    confusion_mat = confusion_matrix(y_test, y_pred)

    # Classification report
    class_report = classification_report(y_test, y_pred)

    # Output the results
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:")
    print(confusion_mat)
    print("Classification Report:")
    print(class_report)

# Main function
def main():
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)
    train_and_evaluate_nb(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()
=========================================================================================

Question -13

Q13. Problem Statement Liam, a customer retention specialist, wants to predict whether a customer will churn based on their demographics and spending behavior. Write a program that loads a dataset from a user-provided CSV file, preprocesses it by handling missing values and standardizing numerical features. Then, train a Gaussian Naïve Bayes model to classify whether a customer will churn and evaluate its performance using accuracy, confusion matrix, and classification report.

=========================================================================================
import os
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Prompt the user for the dataset filename
file_name = input().strip()

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)


# Display first few rows (optional for debugging)
# print(df.head())

# Preprocess the dataset
def preprocess_data(df):
    # Handling missing values: Fill numerical values with the mean
    for col in df.select_dtypes(include=['number']).columns:
        df[col] = df[col].fillna(df[col].mean())

    # Fill missing categorical values with the most frequent value
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].fillna(df[col].mode()[0])

    # Encoding categorical features using LabelEncoder
    label_encoder = LabelEncoder()
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = label_encoder.fit_transform(df[col])

    # Splitting features (X) and target variable (y)
    X = df.drop(columns=['Churn'])  # Assuming 'Churn' is the target column
    y = df['Churn']

    # Split data into training (80%) and testing (20%) sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Feature scaling - Standardizing numerical features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test

# Train and evaluate the Naïve Bayes model
def train_and_evaluate_nb(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the Gaussian Naïve Bayes model
    model = GaussianNB()
    model.fit(X_train_scaled, y_train)

    # Predict on test set
    y_pred = model.predict(X_test_scaled)

    # Compute accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Compute confusion matrix
    confusion_mat = confusion_matrix(y_test, y_pred)

    # Generate classification report
    class_report = classification_report(y_test, y_pred)

    # Output the results
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:")
    print(confusion_mat)
    print("Classification Report:")
    print(class_report)

# Main function to execute the workflow
def main():
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)
    train_and_evaluate_nb(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()

=========================================================================================

Question -14

Q14. Problem Statement Emma is analyzing customer demographics to categorize spending behaviors. Each customer record includes age, work experience, spending score, and family size. The spending score is mapped to numerical values: Low → 0, Average → 1, High → 2. Given N customer records, Emma wants to classify them into K clusters using the K-Means algorithm. Any record whose Euclidean distance from its cluster center exceeds a given threshold T is marked as an outlier. Write a program to determine the cluster number (1-based index) for each customer or mark them as "Outlier".

=========================================================================================


import os
import sys
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder

# User input for file name, number of clusters (K), and outlier threshold (T)
file_name = input("Enter dataset filename: ").strip()
K = int(input("Enter the number of clusters (K): "))
T = float(input("Enter the outlier threshold (T): "))

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Display first few rows (optional for debugging)
# print(df.head())

# Preprocessing function
def preprocess_data(df):
    # Handling missing values: Fill numerical columns with the mean
    for col in df.select_dtypes(include=['number']).columns:
        df[col] = df[col].fillna(df[col].mean())

    # Encode 'Spending Score' (Low=0, Average=1, High=2)
    spending_mapping = {"Low": 0, "Average": 1, "High": 2}
    df["Spending Score"] = df["Spending Score"].map(spending_mapping)

    # Standardize numerical features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df)

    return X_scaled

# Apply preprocessing
X = preprocess_data(df)

# Perform K-Means clustering
kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)
df["Cluster"] = kmeans.fit_predict(X) + 1  # 1-based index for clusters

# Compute Euclidean distances of each point from its cluster center
distances = np.linalg.norm(X - kmeans.cluster_centers_[df["Cluster"] - 1], axis=1)

# Mark points as "Outlier" if distance > T
df["Cluster"] = np.where(distances > T, "Outlier", df["Cluster"])

# Display the results
print(df[["Cluster"]])

# Save the output to a CSV file
output_file = "customer_clusters.csv"
df.to_csv(output_file, index=False)
print(f"Results saved to {output_file}")

=========================================================================================

Question -15

Q15. Problem Statement Aryan is analyzing climate patterns using a dataset that includes WEATHER_ID, TEMPERATURE, HUMIDITY, and CATEGORY. He wants to apply k-means clustering with four centroids to classify different weather patterns based on temperature and humidity. Write a program to read the dataset, apply k-means clustering using TEMPERATURE and HUMIDITY, and allow Aryan to input new values to predict the corresponding weather condition cluster.

=========================================================================================


import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load the dataset
file_name = input()

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Select features for clustering
X = df[['TEMPERATURE', 'HUMIDITY']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering with 4 centroids
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
df["Cluster"] = kmeans.fit_predict(X_scaled) + 1  # 1-based index

# Predict cluster for new input
temp = float(input("Enter TEMPERATURE: "))
humidity = float(input("Enter HUMIDITY: "))

# Scale the input data
new_data = scaler.transform([[temp, humidity]])
predicted_cluster = kmeans.predict(new_data)[0] + 1  # Convert to 1-based index

print(f"The input values belong to Cluster {predicted_cluster}.")

=========================================================================================

Question -16

Q16. Problem Statement Athulya aims to create a program that predicts whether a student will pass or fail based on their scores in math, science, and history. She loads a dataset of scores and pass/fail statuses from a CSV f ile and trains a neural network using backpropagation. After normalization and mapping of pass/fail labels, the program trains the network to minimize prediction errors. Athulya then prompts the user to input scores for math, science, and history. Using the trained network, the program predicts the student's result as pass or fail.

=========================================================================================

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Load dataset
file_name = input().strip()
file1 = os.path.join(sys.path[0], file_name)
df = pd.read_csv(file1)

# Step 2: Preprocessing
# Select features and target
X = df[['Math', 'Science', 'History']]  # Independent variables (numerical scores)
y = df['Result']  # Target variable ("Pass" or "Fail")

# Normalize scores using MinMaxScaler (scales values between 0 and 1)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Encode labels ("Pass" → 1, "Fail" → 0) , If dataset is already numeric leave this step
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y) # see the dataset carefully to analyse to apply the label encoder are not

# Step 3: Train Neural Network (MLP)
model = MLPClassifier(hidden_layer_sizes=(10, 5), activation='relu', solver='adam', max_iter=1000, random_state=42) # solver ans activation are not necessary , if need use this
model.fit(X_scaled, y_encoded)

# Step 5: Predict Student Result
math_score = float(input("Enter Math score: "))
science_score = float(input("Enter Science score: "))
history_score = float(input("Enter History score: "))

# Normalize input scores using the same scaler
input_data = scaler.transform([[math_score, science_score, history_score]])

# Predict the result
prediction = model.predict(input_data)[0]
predicted_label = label_encoder.inverse_transform([prediction])[0]

print(f"Predicted Result: {predicted_label}")

=========================================================================================

Question -17

Q17. Problem Statement Prawin is a financial analyst working for a credit risk assessment company. His task is to develop a classifier that predicts the credit risk of individuals based on their demographic and financial data. The company has provided him with a dataset containing information about individuals' education, employment status, and credit risk. Prawin needs to build a decision tree classifier to predict credit risk. Additionally, he wants to calculate the Gini impurity of the dataset to assess its purity before and after training the model.

=========================================================================================

import pandas as pd
import numpy as np
import os
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter

# Load the dataset
file_name = input()

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Step 2: Preprocessing
# Fill missing values with the mode for categorical and mean for numerical columns
df.fillna(df.mode().iloc[0], inplace=True)

# Identify categorical and numerical columns
categorical_columns = df.select_dtypes(include=['object']).columns
numerical_columns = df.select_dtypes(include=['number']).columns

# Convert categorical columns using Label Encoding
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # Save encoders for future use

# Scale numerical features (optional but improves accuracy)
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Select features and target
X = df.drop(columns=['CreditRisk'])  # Independent variables
y = df['CreditRisk']  # Target variable (Credit Risk: "Low", "Medium", "High")

# Encode CreditRisk labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Convert to numerical labels

# Step 3: Calculate Gini Impurity of the dataset before training
def gini_impurity(y):
    total = len(y)
    if total == 0:
        return 0
    counts = Counter(y)
    impurity = 1 - sum((count / total) ** 2 for count in counts.values())
    return impurity

gini_before = gini_impurity(y_encoded)
print(f"Gini Impurity before training: {gini_before:.4f}")

# Step 4: Train Decision Tree Classifier
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42) # if testcase dosesn't pass take off this testtrain split
clf = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42)  # Added max_depth for better generalization, gini and max_depth are not necessary , but you can add
clf.fit(X_train, y_train)

# Step 5: Evaluate the Model
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Model Accuracy: {accuracy:.2f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Step 6: Calculate Gini Impurity after training (on predicted values)
gini_after = gini_impurity(y_pred)
print(f"Gini Impurity after training: {gini_after:.4f}")

# Step 7: Predict Credit Risk for a new individual
print("\nEnter details to predict credit risk:")
education_input = input("Enter Education level: ").strip()
employment_input = input("Enter Employment status: ").strip()

# Convert input using previously fitted encoders (handle unknown inputs)
try:
    education_encoded = label_encoders['Education'].transform([education_input])[0]
    employment_encoded = label_encoders['Employment'].transform([employment_input])[0]
except ValueError:
    print("Error: Invalid category entered. Please enter a valid Education or Employment status.")
    sys.exit(1)

# Make prediction
new_data = np.array([[education_encoded, employment_encoded]])
predicted_risk = clf.predict(new_data)[0]
predicted_label = label_encoder.inverse_transform([predicted_risk])[0]

print(f"Predicted Credit Risk: {predicted_label}")


=========================================================================================

Question -18

Q18. Problem Statement As a financial analyst, Ramesh is responsible for evaluating loan applications. He has received a dataset containing details of applicants, including demographics, income levels, loan amounts, and credit histories. His objective is to preprocess the dataset by handling missing values, applying different imputation techniques, and scaling loan amounts for further analysis. Identify and Report Missing Data: • Read a CSV file containing loan application records. • Identify missing values in each column and report the count of missing values. Handle Missing Data: • Remove rows that contain missing values. • Fill missing values in "LoanAmount" using the mean. • Fill missing values in "Loan_Term" using the most frequent value. • For categorical columns like "Gender," "Credit_History," and "Loan_Status," replace missing values with the mode. Standardization and Normalization: • Standardize the "LoanAmount" column using Z-score normalization (StandardScaler). • Normalize the "LoanAmount" column using Min-Max scaling (MinMaxScaler). Display Datasets After Handling Missing Data: • Show the dataset after dropping rows with missing values. • Display the standardized and normalized values of the "LoanAmount" column. Help Ramesh to achieve this tas

=========================================================================================

import pandas as pd
import os
import sys
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Load the dataset
file_name = input()

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Step 1: Identify and Report Missing Data
print("\nMissing Data Count Before Handling:")
missing_values = df.isnull().sum()
print(missing_values)

# Step 2: Handle Missing Data

# Remove rows with missing values
df_dropped = df.dropna()
print(f"\nDataset after dropping missing values (rows removed: {len(df) - len(df_dropped)}):")
print(df_dropped.isnull().sum())  # Should print zero missing values

# Create a copy of the dataset for imputation
df_imputed = df.copy()

# Fill missing values in "LoanAmount" using mean
df_imputed["LoanAmount"].fillna(df["LoanAmount"].mean(), inplace=True)

# Fill missing values in "Loan_Term" using the most frequent value (mode)
df_imputed["Loan_Term"].fillna(df["Loan_Term"].mode()[0], inplace=True)
=========================================================================================

Question -1

Q1. Problem Statement Alex is working on a navigation system that finds the shortest path between two locations using the A* algorithm. Given a directed graph where nodes represent locations and edges represent paths with weights, along with heuristic values for each node, write a program to determine the optimal path from a starting node to a destination node. If a path exists, print the sequence of nodes in the order they should be visited; otherwise, print that no path exists.

=========================================================================================

import heapq , defaultdict

def from_to_cost_edges(edges_dummy):
    edges = defaultdict(list)
    for frm, to, cost in edges_dummy:
        edges[frm].append((to, cost))
    return edges

def tuple_to_heuritic(heuristic_dummy):
    heuristic = {}
    for node, h_cost in heuristic_dummy:
        heuristic[node] = h_cost
    return heuristic

def a_star(edges, heuristic, start, goal):
    priority_queue = []

    #Kept the f-cost in o index then only sorting will happen inside the heap easily
    heapq.heappush(priority_queue, (0 + heuristic[start], 0, start, []))  # (f-cost, g-cost, node, path)

    visited = set()

    while priority_queue:
        f_cost, g_cost, current, path = heapq.heappop(priority_queue)

        if current in visited:
            continue
        visited.add(current)

        path = path + [current]

        if current == goal:
            return path, g_cost

        for neighbor, cost in edges.get(current, []):
            if neighbor not in visited:
                new_g_cost = g_cost + cost
                new_f_cost = new_g_cost + heuristic.get(neighbor, float('inf'))
                heapq.heappush(priority_queue, (new_f_cost, new_g_cost, neighbor, path))

    return None, float('inf')  # No path found

if __name__ == "__main__":

    n = int(input())  # Number of nodes

    e = int(input())  # Number of edges

    edges_dummy = []
    for _ in range(e):
        frm, to, cost = map(int, input().split())
        edges_dummy.append((frm, to, cost))

    heuristic_dummy = []
    h = int(input())  # Number of heuristic values

    for _ in range(h):
        node, h_cost = map(int, input().split())
        heuristic_dummy.append((node, h_cost))

    start = int(input())  # Start node

    goal = int(input())  # Goal node

    edges = from_to_cost_edges(edges_dummy)
    heuristic = tuple_to_heuritic(heuristic_dummy)

    shortest_path, total_cost = a_star(edges, heuristic, start, goal)

    if shortest_path:
        print("Shortest Path:", shortest_path)
        print("Total Cost:", total_cost)
    else:
        print("No path found")

=========================================================================================

Question -2

Q2. Problem Statement Implement the A* search algorithm to determine the minimum path cost from a starting node to a target node in a directed weighted graph. Each node in the graph has a list of neighbors with corresponding edge costs, and each node also has an associated heuristic value estimating its cost to the goal. If a path exists, output the total path cost; otherwise, output "Path does not exist!".

=========================================================================================
import heapq
import collections

def from_to_cost_edges(edges_dummy):
    edges = collections.defaultdict(list)
    for frm, to, cost in edges_dummy:
        edges[frm].append((to, cost))
    return edges

def tuple_to_heuristic(heuristic_dummy):
    return {node: h_cost for node, h_cost in heuristic_dummy}

def a_star(edges, heuristic, start, goal):
    priority_queue = []
    heapq.heappush(priority_queue, (heuristic[start], 0, start, []))  # (f-cost, g-cost, current_node, path)

    visited = set()

    while priority_queue:
        f_cost, g_cost, current, path = heapq.heappop(priority_queue)

        if current in visited:
            continue

        path = path + [current]
        visited.add(current)

        if current == goal:
            return path, g_cost  # Return the path and total cost

        for neighbor, edge_cost in edges[current]:
            if neighbor not in visited:
                new_g_cost = g_cost + edge_cost
                new_f_cost = new_g_cost + heuristic.get(neighbor, float('inf'))  # f = g + h
                heapq.heappush(priority_queue, (new_f_cost, new_g_cost, neighbor, path))

    return None, None  # No valid path

if __name__ == "__main__":
    n = int(input())  # Number of nodes
    e = int(input())  # Number of edges

    edges_dummy = []
    for _ in range(e):
        frm, to, cost = map(int, input().split())
        edges_dummy.append((frm, to, cost))

    h = int(input())  # Number of heuristic values
    heuristic_dummy = []
    for _ in range(h):
        node, h_cost = map(int, input().split())
        heuristic_dummy.append((node, h_cost))

    start = int(input())  # Start node
    goal = int(input())   # Goal node

    edges = from_to_cost_edges(edges_dummy)
    heuristic = tuple_to_heuristic(heuristic_dummy)

    shortest_path, total_cost = a_star(edges, heuristic, start, goal)

    if shortest_path:
        print("Path:", " -> ".join(map(str, shortest_path)))
        print("Total Cost:", total_cost)
    else:
        print("Path does not exist!")
=========================================================================================

Q3. Problem Statement George, a financial analyst, is working on optimizing the distribution of funds across various investment portfolios. Each investment scenario is represented as a node in a binary decision tree, where each node decides whether to increase or decrease investment in a particular portfolio.

George aims to use a minimax algorithm with alpha-beta pruning to determine the most favorable investment strategy, maximizing returns at various decision points.

=========================================================================================
#Here the premax and premin logic is used to correctly use the printStatements to pass testCase

def minimax(depth_level , depth, node_index, is_maximizing_player, values, alpha, beta):
    # Base case: If the depth is 0, return the value of the leaf node
    if depth == depth_level:
        return values[node_index]

    # If we are at a maximizing player
    if is_maximizing_player:
        max_val = float('-inf')
        pre_max = 0
        for i in range(2):  # Two children of each node
            value = minimax(depth_level + 1,depth ,  node_index * 2 + i , False, values, alpha, beta)
            max_val = max(max_val, value)
            # Update alpha
            if pre_max == 0 or pre_max != max_val:
                print(f"Maximizer updated best value to {max_val} at depth {depth_level}")

            alpha = max(alpha, max_val)

            if beta <= alpha:
                print(f"Pruning at depth {depth_level} by Maximizer")
                break
            pre_max = max_val

        return max_val

    # If we are at a minimizing player
    else:
        min_val = float('inf')
        pre_min = 0
        for i in range(2):  # Two children of each node
            value = minimax(depth_level + 1,depth , node_index * 2 + i , True, values, alpha, beta)
            min_val = min(min_val, value)
            # Update beta
            if pre_min == 0 or pre_min != min_val:
                print(f"Minimizer updated best value to {min_val} at depth {depth_level}")

            beta = min(beta, min_val)
            if beta <= alpha:
                print(f"Pruning at depth {depth_level} by Minimizer")
                break
            pre_min = min_val
        return min_val

def decision_tree(d, leaf_values):
    # Start from depth 0 (root) with alpha as negative infinity and beta as positive infinity
    optimal_value = minimax(0 , d , 0, True, leaf_values, float('-inf'), float('inf'))
    return optimal_value

# Input
d = int(input())  # depth of the tree

leaf_values = list(map(int, input().split()))  # leaf values at the bottom level of the tree

# Note if depth is not given Use this logic to get depth
# d = len(leaf_values).bit_length() - 1

# Output the result
optimal_value = decision_tree(d, leaf_values)
print(f"Optimal Value: {optimal_value}")

=========================================================================================

Q4. Problem Statement Julia, an environmental scientist, is working on optimizing water distribution strategies during periods of scarcity. She is using a decision-making tool that evaluates various strategies for allocating water to different regions based on predicted need and availability.

To improve the strategy, Julia implements a minimax algorithm with alpha-beta pruning within a decision tree, where each node represents a decision on water distribution. Additionally, Julia introduces a flat increase of 10 units of water at each node as a safety buffer, ensuring each area has an emergency reserve.

=========================================================================================

# I think , this Question similar like Question number 1(Refer My Machine Learning LAB Colab) in Alpha Beta Pruning that we done in examly portal.

import math

def alpha_beta_pruning(node, depth, alpha, beta, maximizing_player, water_supply):
    if depth == 0:
        return water_supply[node] + 10  # Adding 10 units as an emergency reserve

    if maximizing_player:
        max_eval = float('-inf')
        for i in range(2):  # Binary tree (each node has 2 children)
            value = alpha_beta_pruning(2 * node + i, depth - 1, alpha, beta, False, water_supply)
            max_eval = max(max_eval, value)
            alpha = max(alpha, max_eval)
            if beta <= alpha:
                break  # Beta cut-off
        return max_eval

    else:
        min_eval = float('inf')
        for i in range(2):
            value = alpha_beta_pruning(2 * node + i, depth - 1, alpha, beta, True, water_supply)
            min_eval = min(min_eval, value)
            beta = min(beta, min_eval)
            if beta <= alpha:
                break  # Alpha cut-off
        return min_eval

# Input: List of 8 integers representing available water at the leaf nodes
water_supply = list(map(int, input("Enter 8 water supply values: ").split()))
depth = len(water_supply).bit_length() - 1  # Depth of the tree

# Get the optimal water allocation
optimal_allocation = alpha_beta_pruning(0, depth, float('-inf'), float('inf'), True, water_supply)

# Output the results
print(f"Optimal water allocation: {optimal_allocation}")
# print(f"After 10% increase: {int(optimal_allocation * 1.10)}") Not Necessary

=========================================================================================

Question -5

Q5. Problem Statement Vino is a data analyst working with a loan dataset that contains information about urban and rural areas. The dataset has missing values that need to be addressed. Vino is also tasked with filling in missing values for the "Employment," "Population," and "Income" columns. Additionally, Vino needs to perform data standardization and normalization for the "Income" column. Handling Missing Data and Standardization: • Load the CSV dataset, and drop rows with missing values. • Fill categorical "Employment" with mode, "Population" with mean, and "Income" with median. • Display filled values. Standardize "Income_filled_median" using StandardScaler and normalize using MinMaxScaler.

=========================================================================================
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import os
import sys

# Load dataset
file_path = os.path.join(sys.path[0] , "urban_rural.csv")
df = pd.read_csv(file_path)

# Display missing data information
missing_data = df.isnull().sum()
print("Missing Data:")
print(missing_data)
print()

# Drop rows with missing values
df_dropna = df.dropna()
print("Dataset after dropping missing values:")
print(df_dropna.to_string(index=True))
print()

# Fill categorical column 'Employment' with mode
employment_imputer = SimpleImputer(strategy='most_frequent')
df['Employment_filled'] = employment_imputer.fit_transform(df[['Employment']])
print("Filled values for Employment with mode:")
print(df['Employment_filled'])
print()

# Fill numerical column 'Population' with mean
population_imputer = SimpleImputer(strategy='mean')
df['Population_filled_mean'] = population_imputer.fit_transform(df[['Population']])
print("Filled values for Population with mean:")
print(df['Population_filled_mean'])
print()

# Fill numerical column 'Income' with median
income_imputer = SimpleImputer(strategy='median')
df['Income_filled_median'] = income_imputer.fit_transform(df[['Income']])
print("Filled values for Income with median:")
print(df['Income_filled_median'])
print()

# Standardize 'Income_filled_median'
scaler = StandardScaler()
df['Income_Standardized'] = scaler.fit_transform(df[['Income_filled_median']])
print("Standardized Income:")
print(df[['Income_Standardized']])
print()

# Normalize 'Income_filled_median'
minmax_scaler = MinMaxScaler()
df['Income_Normalized'] = minmax_scaler.fit_transform(df[['Income_filled_median']])
print("Normalized Income:")
print(df[['Income_Normalized']])


#Without SimpleImputer

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import os
import sys

# Load dataset
file_path = os.path.join(sys.path[0], "urban_rural.csv")
df = pd.read_csv(file_path)

# Display missing data information
missing_data = df.isnull().sum()
print("Missing Data:")
print(missing_data)
print()

# Drop rows with missing values
df_dropna = df.dropna()
print("Dataset after dropping missing values:")
print(df_dropna.to_string(index=True))
print()

# Fill categorical column 'Employment' with mode
df['Employment_filled'] = df['Employment'].fillna(df['Employment'].mode()[0])
print("Filled values for Employment with mode:")
print(df['Employment_filled'])
print()

# Fill numerical column 'Population' with mean
df['Population_filled_mean'] = df['Population'].fillna(df['Population'].mean())
print("Filled values for Population with mean:")
print(df['Population_filled_mean'])
print()

# Fill numerical column 'Income' with median
df['Income_filled_median'] = df['Income'].fillna(df['Income'].median())
print("Filled values for Income with median:")
print(df['Income_filled_median'])
print()

# Standardize 'Income_filled_median'
scaler = StandardScaler()
df['Income_Standardized'] = scaler.fit_transform(df[['Income_filled_median']])
print("Standardized Income:")
print(df[['Income_Standardized']])
print()

# Normalize 'Income_filled_median'
minmax_scaler = MinMaxScaler()
df['Income_Normalized'] = minmax_scaler.fit_transform(df[['Income_filled_median']])
print("Normalized Income:")
print(df[['Income_Normalized']])

=========================================================================================

Question -6

Q6. Problem Statement Lora, a data analyst, is working on a project involving linear regression analysis. She has a dataset stored in a CSV file, consisting of two columns: 'x' and 'y'. Lora needs to perform linear regression analysis on this dataset to understand the relationship between 'x' and 'y'. To facilitate this analysis, you are tasked with developing a Python program. The program will prompt the user to input the filename of the dataset. It will then read the data from the CSV file, execute linear regression analysis using the Scipy library, and calculate the slope, intercept, and estimated value at x=10. Finally, it will print these results rounded to four decimal places.

=========================================================================================

#As in Question Mentioned I do linearReg with scipy.stats

import pandas as pd
import numpy as np
from scipy.stats import linregress
import os
import sys

# Function to load dataset
def load_dataset(filename):
    return pd.read_csv(filename)

# Function to perform linear regression
def perform_linear_regression(df):
    x = df['x']
    y = df['y']

    # Compute linear regression
    slope, intercept, r_value, p_value, std_err = linregress(x, y)

    # Estimate y when x = 10
    estimated_y = slope * 10 + intercept

    # Print results rounded to 4 decimal places
    print(f"Slope: {slope:.4f}")
    print(f"Intercept: {intercept:.4f}")
    print(f"Estimated value at x=10: {estimated_y:.4f}")

# Main function
def main():
    filename = input().strip()  # Take filename input
    df = load_dataset(os.path.join(sys.path[0], filename))
    perform_linear_regression(df)

if __name__ == "__main__":
    main()

#OtherWay

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
import os
import sys

# Function to load the dataset
def load_dataset(filename):
    return pd.read_csv(filename)

# Function to train and evaluate the linear regression model
def train_and_evaluate_model(df):
    # Selecting features and target variable
    X = df[['x']]
    y = df['y']

    # Creating and training the model
    model = LinearRegression()
    model.fit(X, y)

    print(f"Slope: {model.coef_[0]:.4f}")
    print(f"Intercept: {model.intercept_:.4f}")

    # Predicting the value for x = 10
    estimated_y = model.predict(np.array([[10]]))[0]
    print(f"Estimated value at x=10: {estimated_y:.4f}")

# Main function to execute the program
def main():
    filename = input().strip()  # Taking filename input
    df = load_dataset(os.path.join(sys.path[0], filename))
    train_and_evaluate_model(df)

if __name__ == "__main__":
    main()

=========================================================================================

Question -7

Q7. Problem Statement Paul, a data scientist, is working on a project that involves evaluating the performance of a simple linear regression algorithm. He has a dataset stored in a CSV file, comprising two columns: 'x' and 'y'. Paul aims to calculate the root mean squared error (RMSE) of the regression algorithm applied to this dataset. To assist Paul, you are tasked with creating a Python program. The program will prompt the user to input the filename of the dataset. Subsequently, it will read the data from the CSV file, compute the RMSE using a simple linear regression algorithm, and output the RMSE value rounded to three decimal places.

=========================================================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import os
import sys

# Function to load the dataset
def load_dataset(filename):
    return pd.read_csv(filename)

# Function to train and evaluate the linear regression model
def train_and_evaluate_model(df):
    # Selecting features and target variable
    X = df[['x']]
    y = df['y']

    # Splitting data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Creating and training the model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Making predictions
    y_pred = model.predict(X_test)

    # Calculating RMSE
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    # Printing RMSE rounded to three decimal places
    print(f"Root Mean Squared Error (RMSE): {rmse:.3f}")

# Main function to execute the program
def main():
    filename = input().strip()  # Taking filename input
    df = load_dataset(os.path.join(sys.path[0], filename))
    train_and_evaluate_model(df)

if __name__ == "__main__":
    main()

=========================================================================================

Question -8

Q8. Problem Statement A data scientist is working on a customer churn prediction model for a subscription-based service. The dataset contains various attributes of customers, such as age, sex, account length, number of products used, credit card status, activity level, and estimated salary. The target variable Churn indicates whether a customer has canceled their subscription (1) or not (0). Write a program that reads the dataset from a CSV file, fits a logistic regression model to predict whether a customer will churn based on the given attributes, and evaluates the model's performance using accuracy, precision, recall, F1 score, AUC-ROC score, and confusion matrix. The program should output the evaluation metrics rounded to two decimal places.

=========================================================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import os
import sys

def load_dataset(filename):
    """Loads the dataset from a CSV file."""
    return pd.read_csv(filename)

def preprocess_data(df):
    """Prepares the dataset for training."""
    X = df.drop(columns=['Churn'])  # Features
    y = df['Churn']  # Target variable

    # Standardizing numerical features (optional but helps logistic regression)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return X_scaled, y

def train_and_evaluate_model(X, y):
    """Trains a Logistic Regression model and evaluates it."""
    # Splitting data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Creating and training the model
    model = LogisticRegression()
    model.fit(X_train, y_train)

    # Making predictions
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]  # Probabilities for AUC-ROC

    # Calculating evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    auc_roc = roc_auc_score(y_test, y_prob)
    cm = confusion_matrix(y_test, y_pred)

    # Printing evaluation metrics
    print("Logistic Regression Model Evaluation:")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")
    print(f"AUC-ROC Score: {auc_roc:.2f}")
    print("Confusion Matrix:")
    print(cm)

def main():
    filename = input("Enter the dataset filename: ").strip()
    df = load_dataset(os.path.join(sys.path[0], filename))
    X, y = preprocess_data(df)
    train_and_evaluate_model(X, y)

if __name__ == "__main__":
    main()
=========================================================================================

Question -9

Q9. Problem Statement David, an HR manager, is analyzing employee performance using a dataset that contains hours_worked and a binary target promotion (1 if promoted, 0 otherwise). He needs to predict whether an employee will be promoted based on the number of hours they worked. Write a program that loads the dataset from a CSV file, applies logistic regression using hours_worked as the feature, and calculates the model's precision, recall, F1 score, accuracy, and confusion matrix. The program should output the precision score rounded to four decimal places, along with other metrics.

=========================================================================================

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import os
import sys

# Function to load the dataset
def load_dataset(filename):
    return pd.read_csv(filename)

# Function to train and evaluate the logistic regression model
def train_and_evaluate_model(df):
    # Selecting feature and target variable
    X = df[['hours_worked']]
    y = df['promotion'] # Ref to csv file for correct target name

    # Splitting data into training and testing sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Creating and training the Logistic Regression model
    model = LogisticRegression()
    model.fit(X_train, y_train)

    # Making predictions
    y_pred = model.predict(X_test)

    # Evaluating the model
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)

    # Printing evaluation metrics
    print("Logistic Regression Model Evaluation:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("Confusion Matrix:")
    print(conf_matrix)

# Main function to execute the program
def main():
    filename = input().strip()  # Taking filename input
    df = load_dataset(os.path.join(sys.path[0], filename))
    train_and_evaluate_model(df)

if __name__ == "__main__":
    main()
=========================================================================================

Question -10

Q10. Problem Statement Emily is working on a machine learning project where she needs to classify passenger survival based on Titanic dataset features. Write a program to load a CSV file containing passenger details, preprocess the data by encoding categorical values and handling missing values, train an SVM classifier, and evaluate its performance using accuracy, precision, recall, F1-score, and a confusion matrix. The program should read the CSV filename from user input and output the models evaluation metrics.

=========================================================================================

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Prompt user for the filename
file_name = input("Enter the Titanic dataset filename: ").strip()

# Get the current directory of the script
current_directory = os.path.dirname(__file__)

# Construct the full path to the CSV file
file_path = os.path.join(current_directory, file_name)

# Load the dataset
df = pd.read_csv(file_path)

# Preprocess the dataset
def preprocess_data(df):
    # Selecting relevant columns
    df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]

    # Handle missing values
    df['Age'].fillna(df['Age'].median(), inplace=True)
    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
    df['Fare'].fillna(df['Fare'].median(), inplace=True)

    # Encode categorical variables
    label_encoder = LabelEncoder()
    df['Sex'] = label_encoder.fit_transform(df['Sex'])  # Male=1, Female=0
    df['Embarked'] = label_encoder.fit_transform(df['Embarked'])  # S=2, C=0, Q=1

    # Split features (X) and target (y)
    X = df.drop(columns=['Survived'])
    y = df['Survived']

    # Split into training and test sets (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Feature scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test

# Train and evaluate the SVM model
def train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the SVM model
    svm_model = SVC(kernel='linear', random_state=42)
    svm_model.fit(X_train_scaled, y_train)

    # Predict on test data
    y_pred = svm_model.predict(X_test_scaled)

    # Evaluate model
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)

    # Print results
    print("\nSVM Model Evaluation:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("\nConfusion Matrix:")
    print(conf_matrix)
    print("\nClassification Report:")
    print(class_report)

# Main function to run the program
def main():
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)
    train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()
=========================================================================================

Question -11

Q11. Problem Statement Arjun, a data analyst, wants to automate the process of evaluating a loan approval model using an SVM classifier. Write a program to load a dataset from a given filename, train an SVM classifier with a polynomial kernel, and compute evaluation metrics such as accuracy, precision, recall, and F1-score. The program should handle cases where the dataset contains only one class and exit gracefully with an error message.

=========================================================================================

import os
import pandas as pd
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Prompt the user to enter the filename
file_name = input("Enter the dataset filename: ").strip()

# Get the current directory and construct the full path
current_directory = os.path.dirname(__file__)
file_path = os.path.join(current_directory, file_name)

# Load the dataset
try:
    df = pd.read_csv(file_path)
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    sys.exit(1)

# Display the first few rows (optional)
# print(df.head())

# Preprocess the dataset
def preprocess_data(df):
    # Handling categorical features if any
    label_encoder = LabelEncoder()

    # Encoding categorical columns (if present)
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = label_encoder.fit_transform(df[col])

    # Splitting features (X) and target variable (y)
    X = df.drop(columns=['Loan_Status'])  # Assuming 'Loan_Status' is the target column
    y = df['Loan_Status']

    # Checking if dataset contains only one class
    if len(y.unique()) == 1:
        print("Error: Dataset contains only one class. Classification cannot be performed.")
        sys.exit(1)

    # Splitting the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Feature scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test

# Train and evaluate the SVM model
def train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the SVM classifier with a polynomial kernel
    svm_model = SVC(kernel='poly', degree=3, random_state=42)
    svm_model.fit(X_train_scaled, y_train)

    # Predict on the test data
    y_pred = svm_model.predict(X_test_scaled)

    # Compute evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, zero_division=1)
    recall = recall_score(y_test, y_pred, zero_division=1)
    f1 = f1_score(y_test, y_pred, zero_division=1)

    # Output the results
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

# Main function
def main():
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)
    train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()
=========================================================================================

Question -12

Q12. Problem Statement Emma, a retail analyst, needs to predict whether a product will be sold out based on historical sales data. Write a program that loads a dataset from a user-provided CSV file, preprocesses it by handling missing values, encoding categorical features, and scaling numerical features. Then, train a Gaussian Naïve Bayes model to classify whether a product is sold out and evaluate its performance using accuracy, confusion matrix, and classification report.

=========================================================================================

import os
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Prompt the user to enter the filename
file_name = input().strip()

# Construct the full path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Display first few rows (optional)
# print(df.head())

# Preprocess the dataset
def preprocess_data(df):
    # Handling missing values: Fill numerical values with the mean
    for col in df.select_dtypes(include=['number']).columns:
        df[col] = df[col].fillna(df[col].mean())

    # Fill missing categorical values with the most frequent value
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].fillna(df[col].mode()[0])

    # Encoding categorical features
    label_encoder = LabelEncoder()
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = label_encoder.fit_transform(df[col])

    # Splitting features (X) and target variable (y)
    X = df.drop(columns=['Sold_Out'])  # Assuming 'Sold_Out' is the target column
    y = df['Sold_Out']

    scalar = StandardScalar()
    X_scaled = scalar.fit_transform(X)

    # Split data into training and testing sets (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    return X_train, X_test, y_train, y_test

# Train and evaluate the Naïve Bayes model
def train_and_evaluate_nb(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the Gaussian Naïve Bayes model
    model = GaussianNB()
    model.fit(X_train_scaled, y_train)

    # Predict on test set
    y_pred = model.predict(X_test_scaled)

    # Compute accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Compute confusion matrix
    confusion_mat = confusion_matrix(y_test, y_pred)

    # Classification report
    class_report = classification_report(y_test, y_pred)

    # Output the results
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:")
    print(confusion_mat)
    print("Classification Report:")
    print(class_report)

# Main function
def main():
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)
    train_and_evaluate_nb(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()
=========================================================================================

Question -13

Q13. Problem Statement Liam, a customer retention specialist, wants to predict whether a customer will churn based on their demographics and spending behavior. Write a program that loads a dataset from a user-provided CSV file, preprocesses it by handling missing values and standardizing numerical features. Then, train a Gaussian Naïve Bayes model to classify whether a customer will churn and evaluate its performance using accuracy, confusion matrix, and classification report.

=========================================================================================
import os
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Prompt the user for the dataset filename
file_name = input().strip()

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)


# Display first few rows (optional for debugging)
# print(df.head())

# Preprocess the dataset
def preprocess_data(df):
    # Handling missing values: Fill numerical values with the mean
    for col in df.select_dtypes(include=['number']).columns:
        df[col] = df[col].fillna(df[col].mean())

    # Fill missing categorical values with the most frequent value
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].fillna(df[col].mode()[0])

    # Encoding categorical features using LabelEncoder
    label_encoder = LabelEncoder()
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = label_encoder.fit_transform(df[col])

    # Splitting features (X) and target variable (y)
    X = df.drop(columns=['Churn'])  # Assuming 'Churn' is the target column
    y = df['Churn']

    # Split data into training (80%) and testing (20%) sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Feature scaling - Standardizing numerical features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled, X_test_scaled, y_train, y_test

# Train and evaluate the Naïve Bayes model
def train_and_evaluate_nb(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the Gaussian Naïve Bayes model
    model = GaussianNB()
    model.fit(X_train_scaled, y_train)

    # Predict on test set
    y_pred = model.predict(X_test_scaled)

    # Compute accuracy
    accuracy = accuracy_score(y_test, y_pred)

    # Compute confusion matrix
    confusion_mat = confusion_matrix(y_test, y_pred)

    # Generate classification report
    class_report = classification_report(y_test, y_pred)

    # Output the results
    print(f"Accuracy: {accuracy:.4f}")
    print("Confusion Matrix:")
    print(confusion_mat)
    print("Classification Report:")
    print(class_report)

# Main function to execute the workflow
def main():
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(df)
    train_and_evaluate_nb(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()

=========================================================================================

Question -14

Q14. Problem Statement Emma is analyzing customer demographics to categorize spending behaviors. Each customer record includes age, work experience, spending score, and family size. The spending score is mapped to numerical values: Low → 0, Average → 1, High → 2. Given N customer records, Emma wants to classify them into K clusters using the K-Means algorithm. Any record whose Euclidean distance from its cluster center exceeds a given threshold T is marked as an outlier. Write a program to determine the cluster number (1-based index) for each customer or mark them as "Outlier".

=========================================================================================


import os
import sys
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder

# User input for file name, number of clusters (K), and outlier threshold (T)
file_name = input("Enter dataset filename: ").strip()
K = int(input("Enter the number of clusters (K): "))
T = float(input("Enter the outlier threshold (T): "))

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Display first few rows (optional for debugging)
# print(df.head())

# Preprocessing function
def preprocess_data(df):
    # Handling missing values: Fill numerical columns with the mean
    for col in df.select_dtypes(include=['number']).columns:
        df[col] = df[col].fillna(df[col].mean())

    # Encode 'Spending Score' (Low=0, Average=1, High=2)
    spending_mapping = {"Low": 0, "Average": 1, "High": 2}
    df["Spending Score"] = df["Spending Score"].map(spending_mapping)

    # Standardize numerical features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df)

    return X_scaled

# Apply preprocessing
X = preprocess_data(df)

# Perform K-Means clustering
kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)
df["Cluster"] = kmeans.fit_predict(X) + 1  # 1-based index for clusters

# Compute Euclidean distances of each point from its cluster center
distances = np.linalg.norm(X - kmeans.cluster_centers_[df["Cluster"] - 1], axis=1)

# Mark points as "Outlier" if distance > T
df["Cluster"] = np.where(distances > T, "Outlier", df["Cluster"])

# Display the results
print(df[["Cluster"]])

# Save the output to a CSV file
output_file = "customer_clusters.csv"
df.to_csv(output_file, index=False)
print(f"Results saved to {output_file}")

=========================================================================================

Question -15

Q15. Problem Statement Aryan is analyzing climate patterns using a dataset that includes WEATHER_ID, TEMPERATURE, HUMIDITY, and CATEGORY. He wants to apply k-means clustering with four centroids to classify different weather patterns based on temperature and humidity. Write a program to read the dataset, apply k-means clustering using TEMPERATURE and HUMIDITY, and allow Aryan to input new values to predict the corresponding weather condition cluster.

=========================================================================================


import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load the dataset
file_name = input()

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Select features for clustering
X = df[['TEMPERATURE', 'HUMIDITY']]

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering with 4 centroids
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
df["Cluster"] = kmeans.fit_predict(X_scaled) + 1  # 1-based index

# Predict cluster for new input
temp = float(input("Enter TEMPERATURE: "))
humidity = float(input("Enter HUMIDITY: "))

# Scale the input data
new_data = scaler.transform([[temp, humidity]])
predicted_cluster = kmeans.predict(new_data)[0] + 1  # Convert to 1-based index

print(f"The input values belong to Cluster {predicted_cluster}.")

=========================================================================================

Question -16

Q16. Problem Statement Athulya aims to create a program that predicts whether a student will pass or fail based on their scores in math, science, and history. She loads a dataset of scores and pass/fail statuses from a CSV f ile and trains a neural network using backpropagation. After normalization and mapping of pass/fail labels, the program trains the network to minimize prediction errors. Athulya then prompts the user to input scores for math, science, and history. Using the trained network, the program predicts the student's result as pass or fail.

=========================================================================================

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Load dataset
file_name = input().strip()
file1 = os.path.join(sys.path[0], file_name)
df = pd.read_csv(file1)

# Step 2: Preprocessing
# Select features and target
X = df[['Math', 'Science', 'History']]  # Independent variables (numerical scores)
y = df['Result']  # Target variable ("Pass" or "Fail")

# Normalize scores using MinMaxScaler (scales values between 0 and 1)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Encode labels ("Pass" → 1, "Fail" → 0) , If dataset is already numeric leave this step
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y) # see the dataset carefully to analyse to apply the label encoder are not

# Step 3: Train Neural Network (MLP)
model = MLPClassifier(hidden_layer_sizes=(10, 5), activation='relu', solver='adam', max_iter=1000, random_state=42) # solver ans activation are not necessary , if need use this
model.fit(X_scaled, y_encoded)

# Step 5: Predict Student Result
math_score = float(input("Enter Math score: "))
science_score = float(input("Enter Science score: "))
history_score = float(input("Enter History score: "))

# Normalize input scores using the same scaler
input_data = scaler.transform([[math_score, science_score, history_score]])

# Predict the result
prediction = model.predict(input_data)[0]
predicted_label = label_encoder.inverse_transform([prediction])[0]

print(f"Predicted Result: {predicted_label}")

=========================================================================================

Question -17

Q17. Problem Statement Prawin is a financial analyst working for a credit risk assessment company. His task is to develop a classifier that predicts the credit risk of individuals based on their demographic and financial data. The company has provided him with a dataset containing information about individuals' education, employment status, and credit risk. Prawin needs to build a decision tree classifier to predict credit risk. Additionally, he wants to calculate the Gini impurity of the dataset to assess its purity before and after training the model.

=========================================================================================

import pandas as pd
import numpy as np
import os
import sys
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from collections import Counter

# Load the dataset
file_name = input()

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Step 2: Preprocessing
# Fill missing values with the mode for categorical and mean for numerical columns
df.fillna(df.mode().iloc[0], inplace=True)

# Identify categorical and numerical columns
categorical_columns = df.select_dtypes(include=['object']).columns
numerical_columns = df.select_dtypes(include=['number']).columns

# Convert categorical columns using Label Encoding
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # Save encoders for future use

# Scale numerical features (optional but improves accuracy)
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Select features and target
X = df.drop(columns=['CreditRisk'])  # Independent variables
y = df['CreditRisk']  # Target variable (Credit Risk: "Low", "Medium", "High")

# Encode CreditRisk labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Convert to numerical labels

# Step 3: Calculate Gini Impurity of the dataset before training
def gini_impurity(y):
    total = len(y)
    if total == 0:
        return 0
    counts = Counter(y)
    impurity = 1 - sum((count / total) ** 2 for count in counts.values())
    return impurity

gini_before = gini_impurity(y_encoded)
print(f"Gini Impurity before training: {gini_before:.4f}")

# Step 4: Train Decision Tree Classifier
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42) # if testcase dosesn't pass take off this testtrain split
clf = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42)  # Added max_depth for better generalization, gini and max_depth are not necessary , but you can add
clf.fit(X_train, y_train)

# Step 5: Evaluate the Model
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Model Accuracy: {accuracy:.2f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Step 6: Calculate Gini Impurity after training (on predicted values)
gini_after = gini_impurity(y_pred)
print(f"Gini Impurity after training: {gini_after:.4f}")

# Step 7: Predict Credit Risk for a new individual
print("\nEnter details to predict credit risk:")
education_input = input("Enter Education level: ").strip()
employment_input = input("Enter Employment status: ").strip()

# Convert input using previously fitted encoders (handle unknown inputs)
try:
    education_encoded = label_encoders['Education'].transform([education_input])[0]
    employment_encoded = label_encoders['Employment'].transform([employment_input])[0]
except ValueError:
    print("Error: Invalid category entered. Please enter a valid Education or Employment status.")
    sys.exit(1)

# Make prediction
new_data = np.array([[education_encoded, employment_encoded]])
predicted_risk = clf.predict(new_data)[0]
predicted_label = label_encoder.inverse_transform([predicted_risk])[0]

print(f"Predicted Credit Risk: {predicted_label}")


=========================================================================================

Question -18

Q18. Problem Statement As a financial analyst, Ramesh is responsible for evaluating loan applications. He has received a dataset containing details of applicants, including demographics, income levels, loan amounts, and credit histories. His objective is to preprocess the dataset by handling missing values, applying different imputation techniques, and scaling loan amounts for further analysis. Identify and Report Missing Data: • Read a CSV file containing loan application records. • Identify missing values in each column and report the count of missing values. Handle Missing Data: • Remove rows that contain missing values. • Fill missing values in "LoanAmount" using the mean. • Fill missing values in "Loan_Term" using the most frequent value. • For categorical columns like "Gender," "Credit_History," and "Loan_Status," replace missing values with the mode. Standardization and Normalization: • Standardize the "LoanAmount" column using Z-score normalization (StandardScaler). • Normalize the "LoanAmount" column using Min-Max scaling (MinMaxScaler). Display Datasets After Handling Missing Data: • Show the dataset after dropping rows with missing values. • Display the standardized and normalized values of the "LoanAmount" column. Help Ramesh to achieve this tas

=========================================================================================

import pandas as pd
import os
import sys
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Load the dataset
file_name = input()

# Construct the full file path using sys.path[0] (script's directory)
file_path = os.path.join(sys.path[0], file_name)

df = pd.read_csv(file_path)

# Step 1: Identify and Report Missing Data
print("\nMissing Data Count Before Handling:")
missing_values = df.isnull().sum()
print(missing_values)

# Step 2: Handle Missing Data

# Remove rows with missing values
df_dropped = df.dropna()
print(f"\nDataset after dropping missing values (rows removed: {len(df) - len(df_dropped)}):")
print(df_dropped.isnull().sum())  # Should print zero missing values

# Create a copy of the dataset for imputation
df_imputed = df.copy()

# Fill missing values in "LoanAmount" using mean
df_imputed["LoanAmount"].fillna(df["LoanAmount"].mean(), inplace=True)

# Fill missing values in "Loan_Term" using the most frequent value (mode)
df_imputed["Loan_Term"].fillna(df["Loan_Term"].mode()[0], inplace=True)

# Fill missing values in categorical columns with their mode
categorical_columns = ["Gender", "Credit_History", "Loan_Status"]
for col in categorical_columns:
    df_imputed[col].fillna(df[col].mode()[0], inplace=True)

# Step 3: Standardization and Normalization of "LoanAmount"

# Standardization (Z-score Normalization)
scaler = StandardScaler()
df_imputed['LoanAmount_Standardized'] = scaler.fit_transform(df_imputed[['LoanAmount']])

# Min-Max Normalization
normalizer = MinMaxScaler()
df_imputed['LoanAmount_Normalized'] = normalizer.fit_transform(df_imputed[['LoanAmount']])

# Step 4: Display Results
print("\nDataset after handling missing values:")
print(df_imputed.isnull().sum())  # Should print zero missing values

print("\nStandardized Loan Amount:")
print(df_imputed[['LoanAmount_Standardized']])

print("\nNormalized Loan Amount:")
print(df_imputed[['LoanAmount_Normalized']])


=========================================================================================
=========================================================================================
=========================================================================================
=========================================================================================
=========================================================

			PAT PORTAL SKILL BUILDER AND CODING CHALLENGE 

=========================================================
=========================================================
=========================================================
			 	SKILL BUILDER : WEEK 1
=========================================================
=========================================================
1)

1. Given a directed graph and two nodes (source and destination), the goal is to find all
possible paths from the source node to the destination node.
Consider the following directed graph. Let the s be 2 and d be 3. There are 3
different paths from 2 to 3.

Input Format
 The first line contains an integer n, representing the number of vertices in the
graph.
 The second line contains an integer m, representing the number of edges in
the graph.
 The next m lines each contain two space-separated integers u and v,
representing a directed edge from vertex u to vertex v.
 The next line contains an integer representing the source vertex s.
 The final line contains an integer representing the destination vertex d.
Output Format
 The output prints all distinct paths from the source vertex s to the destination
vertex d, each path should be printed in a separate line as a list of vertices.
The paths should be printed in the order they are found.
Refer to the sample output for the formatting specifications.
Constraints
2 ≤ n ≤ 15
0 ≤ m ≤ n×(n−1)
0 ≤ u, v &lt; n
Sample Input
4
6
0 1
0 2
0 3
2 0
2 1
1 3
2
3
Sample Output

[2, 0, 1, 3]
[2, 0, 3]
[2, 1, 3]

def findLongestPath(grid, rows, cols):
    def dfs(x, y, visited):
        visited.add((x, y))
        max_len = 1
        for dx, dy in [(-1,0),(1,0),(0,-1),(0,1)]:
            nx, ny = x+dx, y+dy
            if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited and grid[nx][ny] != grid[x][y]:
                max_len = max(max_len, 1 + dfs(nx, ny, visited))
        visited.remove((x, y))
        return max_len

    max_path = 0
    for i in range(rows):
        for j in range(cols):
            max_path = max(max_path, dfs(i, j, set()))
    return max_path

# Input handling
M, N = map(int, input().split())
grid = [list(input().strip()) for _ in range(M)]
print(findLongestPath(grid, M, N))



=========================================================
=========================================================

2.) 
You are designing a navigation system for a city with multiple interconnected cities.
The cities are represented as vertices, and the connections between cities are
represented as edges in a graph.
Your task is to implement a program that performs a breadth-first search (BFS)
traversal on the graph, starting from a given source city, and outputs the order in
which the cities are visited.
Input Format
 The first line of input consists of two space-separated integers, v, and e,
representing the total number of cities and the number of connections
between cities.
 The next e lines consist of the two space-separated integers a and b,
representing a connection from city a to city b.
 The last line consists of a single integer s, representing the source city from
which BFS traversal should start.
Output Format
 The output prints the order in which the cities are visited during the BFS
traversal, separated by a space.
Refer to the sample output for formatting specifications and Constraints
In this scenario, the given test cases will fall under the following constraints:
1 ≤ v ≤ 100
0 ≤ e ≤ v * (v - 1)
0 ≤ s &lt; v
Sample Input
6 7
0 1
0 2
1 3
1 4
2 4
3 5
4 5
0
Sample Output
0 1 2 3 4 5

def longestConsecutivePath(matrix, start_char):
    n, m = len(matrix), len(matrix[0])
    dp = [[-1]*m for _ in range(n)]

    def is_valid(x, y):
        return 0 <= x < n and 0 <= y < m

    def dfs(x, y):
        if dp[x][y] != -1:
            return dp[x][y]
        max_len = 1
        for dx, dy in [(-1,0),(1,0),(0,-1),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]:
            nx, ny = x+dx, y+dy
            if is_valid(nx, ny) and ord(matrix[nx][ny]) == ord(matrix[x][y]) + 1:
                max_len = max(max_len, 1 + dfs(nx, ny))
        dp[x][y] = max_len
        return max_len

    result = 0
    for i in range(n):
        for j in range(m):
            if matrix[i][j] == start_char:
                result = max(result, dfs(i, j))
    return result

# Input handling
n = int(input())
m = int(input())
matrix = [input().strip().split() for _ in range(n)]
start = input().strip()
res = longestConsecutivePath(matrix, start)
print(f"The length of the longest path with consecutive characters starting from character {start} is {res}")




=========================================================
=========================================================
3.)

Given a graph G consisting of N nodes, a source S, and an array edges[][2] of type
{u, v} that denotes that there is an undirected edge between node u and v, the task is
to traverse the graph in lexicographical order using a Breadth-First Search(BFS)
algorithm.

Input:
N = 10, M = 10, S = ‘a’,

edges[][2] = { { ‘a’, ‘y’ }, { ‘a’, ‘z’ }, { ‘a’, ‘p’ }, { ‘p’, ‘c’ }, { ‘p’, ‘b’ }, { ‘y’, ‘m’ }, { ‘y’, ‘l’ }, {
‘z’, ‘h’ }, { ‘z’, ‘g’ }, { ‘z’, ‘i’ } } 
Output:
a p y z b c l m g h i
Input Format
 The first line of input is the number of nodes N in the graph.
 The second line consists of the number of edges M of the graph.
 The following lines consist of the edge information.
 The last line is the source character S through which the graph has to be
traversed.
Output Format
 The output is the lexicographical order of traversal using BFS.
Refer to the sample output for formatting specifications and Constraints
1 ≤ N, M ≤ 10
Sample Input
10
10
a y
a z
a p
p c
p b
y m
y l
z h
z g
z i
a
Sample Output
a p y z b c l m g h i

from collections import defaultdict, deque

def bfs_lex_order(n, m, edges, source):
    graph = defaultdict(list)
    for u, v in edges:
        graph[u].append(v)
        graph[v].append(u)

    for node in graph:
        graph[node].sort()  # Sort neighbors for lexicographical order

    visited = set()
    queue = deque([source])
    visited.add(source)
    output = []

    while queue:
        current = queue.popleft()
        output.append(current)
        for neighbor in graph[current]:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append(neighbor)
    print(' '.join(output))

# Input Handling
n = int(input())
m = int(input())
edges = [tuple(input().strip().split()) for _ in range(m)]
source = input().strip()
bfs_lex_order(n, m, edges, source)


=========================================================
=========================================================
				Challenge Yourself: WEEK 1
=========================================================
=========================================================
1) 
Single File Programming Question
Problem Statement 



You are given a grid consisting of characters. Each character represents a different type of node. Implement a program to find the length of the longest path between two nodes of different types in the grid. The path can only move vertically or horizontally, not diagonally.



The grid is represented as a 2D array, with the number of rows and columns specified as input. 



Each cell in the grid contains a single character. To move from one cell to another, you can only go to adjacent cells that share a side (up, down, left, or right). You cannot visit a cell more than once in a single path.



Write a function findLongestPath that takes the grid, the number of rows, and the number of columns as inputs and returns the length of the longest path between two nodes of different types.



Example



Input: 

3 4

ABCD

EFGH

IJKL



Output: 

12



Explanation:

The longest path in this case starts at the cell (0, 0) with the character 'A' and traverses the grid as follows:

(0, 0) -> (0, 1) -> (0, 2) -> (0, 3) -> (1, 3) -> (2, 3) -> (2, 2) -> (2, 1) -> (2, 0) -> (1, 0) -> (1, 1) -> (1, 2)

The path length is 12, which includes visiting all the cells in the grid.

Input format :
The first line of input consists of two space-separated integers: M and N, representing the dimensions of the grid.

The following M lines contain N characters each, representing the characters in the grid.

Output format :
The output prints an integer representing the length of the longest path.



Refer to the sample output for the formatting specifications.

Code constraints :
1 ≤ M, N ≤ 100

The characters in the grid will be uppercase alphabets (A-Z).

The grid will have at least two different types of nodes.

The grid will not contain any spaces or special characters.

Sample test cases :
Input 1 :
3 4
ABCD
EFGH
IJKL
Output 1 :
12
Input 2 :
2 2
AB
CD
Output 2 :
4
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

def findLongestPath(grid, rows, cols):
    def dfs(x, y, visited):
        visited.add((x, y))
        max_len = 1
        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:
            nx, ny = x + dx, y + dy
            if 0 <= nx < rows and 0 <= ny < cols and (nx, ny) not in visited:
                if grid[nx][ny] != grid[x][y]:
                    max_len = max(max_len, 1 + dfs(nx, ny, visited))
        visited.remove((x, y))
        return max_len

    result = 0
    for i in range(rows):
        for j in range(cols):
            result = max(result, dfs(i, j, set()))
    return result

# Input Handling
M, N = map(int, input().split())
grid = [list(input().strip()) for _ in range(M)]

# Output
print(findLongestPath(grid, M, N))



=========================================================
=========================================================
2)
Single File Programming Question
Problem Statement



Given an M × N matrix of characters, find the length of the longest path in the matrix starting from a given character. 



In alphabetical order, all characters in the longest path should be increasing and consecutive. We are allowed to search the string in all eight possible directions, i.e., North, West, South, East, North-East, North-West, South-East, and South-West. 



Use depth-first search (DFS) to solve this problem.



For example, consider the following matrix of characters:



The length of the longest path with consecutive characters starting from character C is 6. 



The longest path is:



Input format :
The first line of input is an integer n representing the row size.

The second line of input is an integer m representing the column size.

The following lines of input are the characters of the matrix.

The last line of the input is the character for which the length of the longest path is to be found.

Output format :
The output prints the length of the longest path with consecutive characters starting from the given character.



Refer to the sample output for formatting specifications.

Code constraints :
1 ≤ n,m ≤ 5

Sample test cases :
Input 1 :
5
5
d e h x b
a o g p e
d d c f d
e b e a s
c d y e n
c
Output 1 :
The length of the longest path with consecutive characters starting from character c is 6
Input 2 :
5
5
d e h x b
a o g p e
d d c f d
e b e a s
c d y e n
b
Output 2 :
The length of the longest path with consecutive characters starting from character b is 7
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

def longestConsecutivePath(matrix, start_char):
    n = len(matrix)
    m = len(matrix[0])
    dp = [[-1 for _ in range(m)] for _ in range(n)]

    def is_valid(x, y):
        return 0 <= x < n and 0 <= y < m

    def dfs(x, y):
        if dp[x][y] != -1:
            return dp[x][y]
        max_len = 1
        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1), (-1,-1), (-1,1), (1,-1), (1,1)]:
            nx, ny = x + dx, y + dy
            if is_valid(nx, ny) and ord(matrix[nx][ny]) == ord(matrix[x][y]) + 1:
                max_len = max(max_len, 1 + dfs(nx, ny))
        dp[x][y] = max_len
        return dp[x][y]

    max_path = 0
    for i in range(n):
        for j in range(m):
            if matrix[i][j] == start_char:
                max_path = max(max_path, dfs(i, j))
    return max_path

# Input Handling
n = int(input())
m = int(input())
matrix = []
for _ in range(n):
    matrix.append(input().strip().split())
start_char = input().strip()

# Output
length = longestConsecutivePath(matrix, start_char)
print(f"The length of the longest path with consecutive characters starting from character {start_char} is {length}")


=========================================================
=========================================================
=========================================================
=========================================================
			 	SKILL BUILDER : WEEK 2
=========================================================
=========================================================
1)
Single File Programming Question
Problem Statement



Elena, a data scientist focusing on signal processing, needs to find the local maximum of a trigonometric function using the hill-climbing algorithm in Prolog. Her task involves optimizing parameters for signal modulation techniques where the trigonometric function characteristics play a crucial role.



Help Elena determine the local maximum of a sine function modified by a quadratic term.



Function Definition: The function to be maximized is defined as Y = sin(X) − 0.1*X2



Note: The search should start at X = 0. Use a small step increment of 0.05 to explore points near the current point on the function curve.

Input format :
No console input.

Output format :
The output displays the x-coordinate where the maximum value is found along with the corresponding y-value.



Refer to the sample output for formatting specifications.

Sample test cases :
Input 1 :
Output 1 :
Maximum found at X = 1.30 with value Y = 0.79
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

import math

def evaluate(x):
    return math.sin(x) - 0.1 * x * x

def hill_climb(start=0.0, step=0.05):
    current_x = start
    current_y = evaluate(current_x)

    while True:
        next_x = current_x + step
        next_y = evaluate(next_x)

        if next_y > current_y:
            current_x = next_x
            current_y = next_y
        else:
            break

    print(f"Maximum found at X = {current_x:.2f} with value Y = {current_y:.2f}")

# Run the function
hill_climb()

=========================================================

2.) 

Single File Programming Question
Problem Statement



You are tasked with implementing the AO* algorithm to find the shortest path in a directed graph, where nodes represent locations, and edges represent the cost to move from one node to another. The nodes and edges of the graph are defined by user input.



The AO* algorithm works by evaluating nodes based on their cumulative cost, choosing the node with the minimum cost to explore next. The goal is to find the shortest path from the start node to the goal node.



You are given a graph with multiple nodes and edges, and your task is to compute the shortest path from the start node to the goal node.

Input format :
The first line of input consists of pairs of node names (strings) and their associated costs (integers), where each node is separated by a space.The input will terminate when the keyword "done" is entered.

The next part of the input consists of relationships between nodes in the format of Parent_Node Child_Node1:Cost1,Child_Node2:Cost2,..., where the parent node is connected to its children with specified costs. The input will terminate when the keyword "done" is entered.

Finally, the starting node and the goal node are provided in two separate lines.

Output format :
The output prints "Shortest Path: StartNode -> IntermediateNode1 -> IntermediateNode2 -> GoalNode" If a path is found from the start node to the goal node.

Otherwise prints "No path found.



Refer to the sample output for the formatting specifications.

Code constraints :
1 ≤ Number of nodes ≤ 10

0 ≤ Cost associated with each node ≤ 10

0 ≤ Number of edges (children) per node ≤ 3

Sample test cases :
Input 1 :
A 0
B 2
C 3
D 5
E 4
done
A B:1,C:2
B D:3,E:1
C E:2
done
A
E
Output 1 :
Shortest Path: A -> B -> E
Input 2 :
X 0
Y 3
Z 4
done
X Y:2,Z:3
done
X
Z
Output 2 :
Shortest Path: X -> Z
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

# ao_star.py

from collections import defaultdict, deque

def ao_star():
    heuristic = {}
    graph = defaultdict(dict)

    while True:
        line = input()
        if line.strip() == "done":
            break
        node, cost = line.split()
        heuristic[node] = int(cost)

    while True:
        line = input()
        if line.strip() == "done":
            break
        parent, children = line.split()
        for pair in children.split(','):
            child, cost = pair.split(':')
            graph[parent][child] = int(cost)

    start = input().strip()
    goal = input().strip()

    def dfs(node, path, cost):
        if node == goal:
            return path + [node], cost
        if node not in graph:
            return None, float('inf')

        min_cost = float('inf')
        best_path = None
        for child in graph[node]:
            new_path, new_cost = dfs(child, path + [node], cost + graph[node][child])
            if new_path and new_cost < min_cost:
                min_cost = new_cost
                best_path = new_path
        return best_path, min_cost

    path, total_cost = dfs(start, [], 0)
    if path:
        print("Shortest Path: " + " -> ".join(path))
    else:
        print("No path found")

ao_star()


=========================================================
=========================================================
3.)
Single File Programming Question
Problem Statement



Klaus is a delivery driver who needs to find the most efficient route to deliver packages to multiple destinations in a city. The city is represented as a graph where intersections are nodes, and roads are edges connecting the nodes. Each road has a cost associated with it, which could represent the distance, time, or fuel consumption. 



Klaus wants to use the A* search algorithm to find the least costly path from his current location to the final destination, considering the estimated cost to reach the goal from each node.



Note: The A* algorithm combines features of uniform-cost search and a greedy best-first search by utilizing both a cost function and a heuristic that estimates the cost from any node to a goal.

Input format :
The first line of input is an integer N, representing the nodes.

The second line contains an integer E, representing the number of edges in the graph.

The next E lines each contain three integers From, To, and Cost, representing an edge with the given cost.

The next line contains an integer H, representing the number of heuristic values provided.

The next H lines each contain two integers Node and Heuristic, representing the heuristic value for the given node.

The last two lines contain integers Start and Goal, representing the start node and the goal node, respectively.

Output format :
The output displays "Visiting: " followed by the nodes visited in order.

The next line of output displays "Goal reached: " followed by the goal reached.

The following line displays "Path: " followed by the path from the start node to the goal node.

The last line of output displays "Cost: " followed by the total cost of the path.



Refer to the sample output for formatting specifications.

Code constraints :
1 ≤ N, E ≤ 10

1 ≤ H ≤ 10

Sample test cases :
Input 1 :
5
6
1 2 1
1 3 4
2 4 2
2 5 5
3 5 1
4 5 3
5
1 7
2 6
3 2
4 1
5 0
1
5
Output 1 :
Visiting: 1
Visiting: 3
Visiting: 5
Goal reached: 5
Path: [1, 3, 5]
Cost: 5
Input 2 :
8
10
1 2 1
1 3 4
2 4 5
3 4 2
3 5 6
4 6 3
4 7 8
5 7 1
6 8 4
7 8 2
8
1 10
2 8
3 7
4 5
5 6
6 3
7 2
8 0
1
8
Output 2 :
Visiting: 1
Visiting: 2
Visiting: 3
Visiting: 4
Visiting: 6
Visiting: 8
Goal reached: 8
Path: [1, 2, 4, 6, 8]
Cost: 13
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

# astar.py

import heapq

def a_star():
    N = int(input())
    E = int(input())
    graph = {i: [] for i in range(1, N + 1)}

    for _ in range(E):
        u, v, cost = map(int, input().split())
        graph[u].append((v, cost))

    H = int(input())
    heuristic = {}
    for _ in range(H):
        node, h = map(int, input().split())
        heuristic[node] = h

    start = int(input())
    goal = int(input())

    open_set = []
    heapq.heappush(open_set, (heuristic[start], 0, start, [start]))
    visited = set()

    while open_set:
        est_total_cost, cost_so_far, current, path = heapq.heappop(open_set)
        print(f"Visiting: {current}")
        if current == goal:
            print(f"Goal reached: {goal}")
            print(f"Path: {path}")
            print(f"Cost: {cost_so_far}")
            return
        if current in visited:
            continue
        visited.add(current)
        for neighbor, edge_cost in graph[current]:
            if neighbor not in visited:
                total_cost = cost_so_far + edge_cost
                est = total_cost + heuristic.get(neighbor, 0)
                heapq.heappush(open_set, (est, total_cost, neighbor, path + [neighbor]))
    print("No path found")

a_star()


=========================================================
=========================================================
				Challenge Yourself: WEEK 2
=========================================================
=========================================================
1) 

Single File Programming Question
Problem Statement



Bob is a delivery person who needs to navigate a warehouse to pick up and deliver packages. The warehouse is represented as a grid where each cell can either be free or have an obstacle.



Your task is to write a program that uses the A* search algorithm to find the shortest path for Bob from his starting position to the goal position in the warehouse grid. The program should output the path Bob needs to take and the cost of the path.



Rules:

Bob can only move up, down, left, or right (no diagonal movement).
Bob cannot move through obstacles.
Each move from one cell to an adjacent free cell has a cost of 1.


Input format :
The first line of input consists of two integers, StartX and StartY, representing the starting position of Bob in the grid.

The second line consists of two integers, GoalX and GoalY, representing the goal position in the grid.

Output format :
The first line of output prints "Path: " followed by the sequence of positions (as tuples) that Bob needs to follow to reach the goal from the start. Each position should be in the format (X, Y).

The second line prints "Cost: " followed by the total cost of the path, which is the number of steps Bob needs to take to reach the goal.



Refer to the sample output for formatting specifications.

Code constraints :
0 ≤ StartX, StartY, GoalX, GoalY ≤ 4

Sample test cases :
Input 1 :
0 0
4 4
Output 1 :
Path: [(0, 0), (0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (4, 2), (4, 3), (4, 4)]
Cost: 8
Input 2 :
1 1
4 4
Output 2 :
Path: [(1, 1), (2, 1), (3, 1), (4, 1), (4, 2), (4, 3), (4, 4)]
Cost: 6
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

import heapq

def heuristic(a, b):
    return abs(a[0] - b[0]) + abs(a[1] - b[1])

def astar(start, goal, grid_size=5, obstacles=set()):
    start = tuple(start)
    goal = tuple(goal)
    open_set = []
    heapq.heappush(open_set, (0 + heuristic(start, goal), 0, start, [start]))
    visited = set()
    
    while open_set:
        f, g, current, path = heapq.heappop(open_set)
        if current == goal:
            print(f"Path: {path}")
            print(f"Cost: {len(path) - 1}")
            return
        if current in visited:
            continue
        visited.add(current)
        x, y = current
        for dx, dy in [(0,1),(1,0),(-1,0),(0,-1)]:
            nx, ny = x + dx, y + dy
            if 0 <= nx < grid_size and 0 <= ny < grid_size and (nx, ny) not in obstacles:
                if (nx, ny) not in visited:
                    heapq.heappush(open_set, (g + 1 + heuristic((nx, ny), goal), g + 1, (nx, ny), path + [(nx, ny)]))

# Input
start_x, start_y = map(int, input().split())
goal_x, goal_y = map(int, input().split())

# Obstacles can be added like {(1,2), (2,2)} if needed
astar((start_x, start_y), (goal_x, goal_y))


=========================================================
=========================================================
2)Single File Programming Question
Problem Statement



Alex is working on implementing the AO (AND-OR)* algorithm to determine the least-cost path through a directed graph. Each node in the graph has an associated heuristic value, representing the cost to reach a goal from that node. The graph can contain both AND and OR conditions:



AND paths require visiting all specified nodes in the condition.
OR paths require selecting the node with the minimum cost.


Given a graph represented by its heuristic values and conditions, your task is to compute the updated cost for each node based on these conditions and determine the shortest path from a given start node to a goal node.

Input format :
The first lines of input contain pairs of node names and heuristic values H, separated by a space. Enter done to end.

For each node, three lines are entered:

Node name.
OR conditions (comma-separated or None or none).
AND conditions (comma-separated or None or none). Enter done to end.
The last input is a single integer w, representing the weight to add to the cost.

Output format :
The first section prints the "Updated Cost:".

Each subsequent line displays the updated cost for a node in the format"

"Node_Name : Path_Conditions >>> Cost_Dictionary" Where Node_Name is the name of the current node.

Path_Conditions specifies the conditions (OR/AND) for the node.Cost_Dictionary is a dictionary showing the evaluated cost for each condition.

The second section prints Shortest Path from the starting node in the format:

"Shortest Path:\n Start_Node = Path_Details".Where Start_Node is the given starting node.Path_Details includes the sequence of nodes, and the conditions (AND/OR) involved.



Refer to the sample output for the formatting specifications.

Code constraints :
1 ≤ heuristic value ≤ 25

1 ≤ number of nodes in AND/OR conditions ≤ 15

1 ≤ W ≤ 10

Sample test cases :
Input 1 :
A 1
B 6
C 2
D 12
E 2
F 1
G 5
H 7
I 7
J 1
T 3
done
A
D
B,C
B
G,H
None
C
J
None
D
None
E,F
G
I
None
done
1
Output 1 :
Updated Cost:
G : {'OR': ['I']} >>> {'I': 8}
D : {'AND': ['E', 'F']} >>> {'E AND F': 5}
C : {'OR': ['J']} >>> {'J': 2}
B : {'OR': ['G', 'H']} >>> {'G OR H': 8}
A : {'OR': ['D'], 'AND': ['B', 'C']} >>> {'B AND C': 12, 'D': 6}
Shortest Path:
 A = D=(E AND F) [E + F]
Input 2 :
A 1
B 6
C 2
done
A
B,C
C,A
done
2
Output 2 :
Updated Cost:
A : {'OR': ['B', 'C'], 'AND': ['C', 'A']} >>> {'C AND A': 7, 'B OR C': 4}
Shortest Path:
 A=(B OR C) [B + C]
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.



def ao_star():
    heuristics = {}
    while True:
        line = input().strip()
        if line.lower() == 'done':
            break
        node, h = line.split()
        heuristics[node] = int(h)

    graph = {}
    while True:
        node = input().strip()
        if node.lower() == 'done':
            break
        or_cond = input().strip()
        and_cond = input().strip()
        or_list = [] if or_cond.lower() in ['none', ''] else [x.strip() for x in or_cond.split(',')]
        and_list = [] if and_cond.lower() in ['none', ''] else [x.strip() for x in and_cond.split(',')]
        graph[node] = {'OR': or_list, 'AND': and_list}
    
    weight = int(input().strip())
    
    updated_costs = {}

    def get_cost(n):
        if n not in graph:
            return heuristics[n]
        costs = {}
        if graph[n]['OR']:
            or_vals = []
            for child in graph[n]['OR']:
                or_vals.append(heuristics[child] + weight)
            costs[" OR ".join(graph[n]['OR'])] = min(or_vals)
        if graph[n]['AND']:
            and_sum = 0
            for child in graph[n]['AND']:
                and_sum += heuristics[child] + weight
            costs[" AND ".join(graph[n]['AND'])] = and_sum
        updated_costs[n] = costs
        heuristics[n] = min(costs.values()) if costs else heuristics[n]
        return heuristics[n]

    for node in reversed(list(graph.keys())):
        get_cost(node)

    print("Updated Cost:")
    for node in updated_costs:
        path_cond = {}
        if graph[node]['OR']:
            path_cond['OR'] = graph[node]['OR']
        if graph[node]['AND']:
            path_cond['AND'] = graph[node]['AND']
        print(f"{node} : {path_cond} >>> {updated_costs[node]}")

    # Shortest path (choose least cost)
    start_node = list(graph.keys())[0]
    costs = updated_costs[start_node]
    best = min(costs, key=costs.get)
    condition = "AND" if "AND" in best else "OR"
    parts = best.split(" AND " if condition == "AND" else " OR ")
    print("Shortest Path:")
    print(f" {start_node} = {best} [{ ' + '.join(parts) }]")
    
ao_star()



=========================================================
=========================================================
=========================================================
=========================================================
			 	SKILL BUILDER : WEEK 3
=========================================================
=========================================================
1)
Single File Programming Question
Problem Statement



George, a strategic planner for emergency management, is designing a decision system to optimize resource allocation during natural disasters. Each decision in his system is represented as a node in a binary decision tree, with choices reflecting different allocation strategies. 



George uses a minimax algorithm with alpha-beta pruning to determine the best strategy that maximizes efficiency while minimizing waste, ensuring optimal resource distribution across affected areas.

Input format :
The first line of input is an integer d, representing the depth of the decision tree.

The second line consists of space-separated integers, representing the values at the leaf nodes of the tree. The number of values should be 2d to ensure a complete binary tree.

Output format :
The output should list each significant action taken during the execution of the minimax algorithm with alpha-beta pruning. Each line should specify whether the action is an update of the best value by the maximizer or minimizer, or a pruning action, along with the depth at which it occurred.

The final line displays the optimal value found by the algorithm.



Refer to the sample output for formatting specifications.

Code constraints :
2 ≤ d ≤ 5

5 ≤ leaf nodes of tree ≤ 100

Sample test cases :
Input 1 :
3
3 5 6 9 1 2 0 -1
Output 1 :
Maximizer updated best value to 3 at depth 2
Maximizer updated best value to 5 at depth 2
Minimizer updated best value to 5 at depth 1
Maximizer updated best value to 6 at depth 2
Pruning at depth 2 by Maximizer
Maximizer updated best value to 5 at depth 0
Maximizer updated best value to 1 at depth 2
Maximizer updated best value to 2 at depth 2
Minimizer updated best value to 2 at depth 1
Pruning at depth 1 by Minimizer
Optimal Value: 5
Input 2 :
2
10 7 2 6
Output 2 :
Minimizer updated best value to 10 at depth 1
Minimizer updated best value to 7 at depth 1
Maximizer updated best value to 7 at depth 0
Minimizer updated best value to 2 at depth 1
Pruning at depth 1 by Minimizer
Optimal Value: 7
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.


# George - Minimax with Alpha-Beta Pruning

def minimax_ab(depth, index, maximizingPlayer, values, alpha, beta, d, output):
    if depth == d:
        return values[index]

    if maximizingPlayer:
        best = float('-inf')
        for i in range(2):
            val = minimax_ab(depth + 1, index * 2 + i, False, values, alpha, beta, d, output)
            if val > best:
                best = val
                output.append(f"Maximizer updated best value to {best} at depth {depth}")
            alpha = max(alpha, best)
            if beta <= alpha:
                output.append(f"Pruning at depth {depth} by Maximizer")
                break
        return best
    else:
        best = float('inf')
        for i in range(2):
            val = minimax_ab(depth + 1, index * 2 + i, True, values, alpha, beta, d, output)
            if val < best:
                best = val
                output.append(f"Minimizer updated best value to {best} at depth {depth}")
            beta = min(beta, best)
            if beta <= alpha:
                output.append(f"Pruning at depth {depth} by Minimizer")
                break
        return best

# Uncomment to test George's problem:
# d = int(input())
# values = list(map(int, input().split()))
# output = []
# result = minimax_ab(0, 0, True, values, float('-inf'), float('inf'), d, output)
# for line in output:
#     print(line)
# print(f"Optimal Value: {result}")


=========================================================

2. 

Single File Programming Question
Problem Statement



Carlos, an agricultural planner, is tasked with optimizing the crop distribution across various fields to maximize yield over the next season. Each decision point in the binary decision tree he's using represents a choice between two different crops for a particular plot of land. 



Carlos wants to employ a minimax algorithm with alpha-beta pruning to determine the best combination of crops that maximizes the total yield. However, in this scenario, the return from each node has an added complexity where the yield benefit from each decision includes a 10% increase due to synergistic effects with neighboring plots.

Input format :
The input contains 8 integers separated by space, representing the base yield (in tons) from each potential crop combination at the leaf nodes.

Output format :
The first line of output displays "Base optimal value: " followed by an integer representing the optimal value calculated without the increase.

The second line displays "After 10% increase: " followed by an integer representing the optimal value after applying a 10% increase to the base optimal value.



Refer to the sample output for formatting specifications.

Code constraints :
20 ≤ base yield≤ 100

Sample test cases :
Input 1 :
30 45 50 55 20 25 40 60
Output 1 :
Base optimal value: 45
After 10% increase: 49
Input 2 :
67 46 90 70 68 30 53 28
Output 2 :
Base optimal value: 67
After 10% increase: 73
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

# Carlos - Minimax with Synergistic 10% Boost

def minimax_simple(depth, index, maximizingPlayer, values, max_depth):
    if depth == max_depth:
        return values[index]

    if maximizingPlayer:
        return max(minimax_simple(depth + 1, index * 2, False, values, max_depth),
                   minimax_simple(depth + 1, index * 2 + 1, False, values, max_depth))
    else:
        return min(minimax_simple(depth + 1, index * 2, True, values, max_depth),
                   minimax_simple(depth + 1, index * 2 + 1, True, values, max_depth))

# Uncomment to test Carlos's problem:
# values = list(map(int, input().split()))
# max_depth = 3  # since 8 leaves
# base_val = minimax_simple(0, 0, True, values, max_depth)
# boosted_val = int(base_val * 1.1)
# print(f"Base optimal value: {base_val}")
# print(f"After 10% increase: {boosted_val}")


=========================================================
=========================================================
3.)
Single File Programming Question
Problem Statement



Oliver is a game developer working on a new strategy game where players need to make optimal decisions at each step to maximize their scores. To test the game's AI, he needs to simulate the Minimax algorithm to find the optimal score from a set of possible outcomes. 



Help Oliver by implementing the Minimax algorithm and taking user input to determine the optimal value at the root node of the decision tree.﻿

Input format :
The first line of input consists of an integer n, representing the number of possible outcomes (which must be a power of 2).

The second line consists of n integers separated by space, representing the scores at the leaf nodes.

Output format :
The output displays "The optimal value is: " followed by an integer representing the optimal value computed by the Minimax algorithm.



Refer to the sample output for formatting specifications.

Code constraints :
4 ≤ n ≤ 32

Sample test cases :
Input 1 :
4
1 2 6 7
Output 1 :
The optimal value is: 6
Input 2 :
8
3 5 2 9 12 5 23 23
Output 2 :
The optimal value is: 12
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

# Oliver - Basic Minimax Algorithm

def minimax(depth, index, maximizingPlayer, values, max_depth):
    if depth == max_depth:
        return values[index]

    if maximizingPlayer:
        return max(minimax(depth + 1, index * 2, False, values, max_depth),
                   minimax(depth + 1, index * 2 + 1, False, values, max_depth))
    else:
        return min(minimax(depth + 1, index * 2, True, values, max_depth),
                   minimax(depth + 1, index * 2 + 1, True, values, max_depth))

# Uncomment to test Oliver's problem:
# n = int(input())
# values = list(map(int, input().split()))
# max_depth = n.bit_length() - 1
# optimal = minimax(0, 0, True, values, max_depth)
# print(f"The optimal value is: {optimal}")


=========================================================
=========================================================
				Challenge Yourself: WEEK 3
=========================================================
=========================================================
1) 
Single File Programming Question
Problem Statement



Sharon is designing a new board game where players must navigate a series of challenges to maximize their points. The game includes both maximizing and minimizing moves, and after finding the optimal score using the Minimax algorithm, players need to multiply this score by a specific multiplier based on the depth of the decision tree.



Help Sharon implement this advanced Minimax algorithm and perform the required operation to determine the final game score.﻿

Input format :
The first line of input consists of an integer N, representing the number of possible outcomes (which must be a power of 2).

The second line consists of N integers separated by space, representing the scores at the leaf nodes.

The third line consists of a floating-point number M, representing the multiplier.

Output format :
The first line of output displays "Optimal value: " followed by an integer representing the optimal value computed by the Minimax algorithm.

The output displays "The final game score is: " followed by a floating-point number representing the final score after applying the multiplier formatted to two decimal places.



Refer to the sample output for formatting specifications.

Code constraints :
4 ≤ N ≤ 32

0.0 ≤ M ≤ 100.0

Sample test cases :
Input 1 :
8
10 9 14 18 5 4 50 3
1.5
Output 1 :
Optimal value: 10
The final game score is: 15.00
Input 2 :
4
12 5 23 24
4.6
Output 2 :
Optimal value: 23
The final game score is: 105.80
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

# Sharon's Game - Minimax with Multiplier
def minimax(depth, index, is_max, values, n):
    if depth == n:
        return values[index]
    if is_max:
        return max(minimax(depth + 1, index * 2, False, values, n),
                   minimax(depth + 1, index * 2 + 1, False, values, n))
    else:
        return min(minimax(depth + 1, index * 2, True, values, n),
                   minimax(depth + 1, index * 2 + 1, True, values, n))

# Read input for Sharon's game
try:
    N = int(input())
    leaf_nodes = list(map(int, input().split()))
    M = float(input())

    # Calculate depth (log2 of N)
    import math
    depth = int(math.log2(N))

    # Run Minimax
    optimal_val = minimax(0, 0, True, leaf_nodes, depth)
    final_score = optimal_val * M

    print(f"Optimal value: {optimal_val}")
    print(f"The final game score is: {final_score:.2f}")
except:
    pass  # Skip if input not provided for this section


=========================================================
=========================================================
2)

Single File Programming Question
Problem Statement



Inez, a traffic manager at a bustling city intersection, needs to optimize signal timings to minimize traffic delays during peak hours. She uses a simplified decision tree model, where each node represents a decision point with different timing adjustments. Each leaf node contains a possible delay time in minutes for a specific setting. 



Inez wants to implement a Minimax algorithm with alpha-beta pruning to find the optimal setting that minimizes the maximum delay after accounting for a fixed 5-minute reduction due to an anticipated improvement in traffic flow.

Input format :
The input consists of 8 space separated integers, representing the delay times in minutes at each leaf node of the decision tree.

Output format :
The first part of the output consists of several lines where each line starts with "Leaf Node at Depth 3: Adjusted Value = ", followed by an integer representing the adjusted delay time after accounting for a 5-minute reduction at each leaf node.

After displaying all leaf nodes, the last line should display "Optimal time saved:" followed by an integer. This integer represents the maximum minimized delay time after the optimal setting is applied.



Refer to the sample output for formatting specifications.

Code constraints :
5 ≤ leaf nodes of tree ≤ 100

Sample test cases :
Input 1 :
3 5 6 9 1 2 0 -1
Output 1 :
Leaf Node at Depth 3: Adjusted Value = -2
Leaf Node at Depth 3: Adjusted Value = 0
Leaf Node at Depth 3: Adjusted Value = 1
Leaf Node at Depth 3: Adjusted Value = -4
Leaf Node at Depth 3: Adjusted Value = -3
Optimal time saved: 0
Input 2 :
100 77 62 96 76 89 78 99
Output 2 :
Leaf Node at Depth 3: Adjusted Value = 95
Leaf Node at Depth 3: Adjusted Value = 72
Leaf Node at Depth 3: Adjusted Value = 57
Leaf Node at Depth 3: Adjusted Value = 91
Leaf Node at Depth 3: Adjusted Value = 71
Leaf Node at Depth 3: Adjusted Value = 84
Optimal time saved: 91
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

# Inez's Game - Minimax with Alpha-Beta Pruning and 5-minute adjustment
def minimax_ab(depth, index, is_maximizing, values, alpha, beta, max_depth):
    if depth == max_depth:
        return values[index]
    
    if is_maximizing:
        best = float('-inf')
        for i in range(2):
            val = minimax_ab(depth + 1, index * 2 + i, False, values, alpha, beta, max_depth)
            best = max(best, val)
            alpha = max(alpha, best)
            if beta <= alpha:
                break
        return best
    else:
        best = float('inf')
        for i in range(2):
            val = minimax_ab(depth + 1, index * 2 + i, True, values, alpha, beta, max_depth)
            best = min(best, val)
            beta = min(beta, best)
            if beta <= alpha:
                break
        return best

# Read input for Inez’s problem
try:
    input_values = list(map(int, input().split()))
    if len(input_values) == 8:
        adjusted = [val - 5 for val in input_values]

        for val in adjusted:
            print(f"Leaf Node at Depth 3: Adjusted Value = {val}")

        depth = 0
        index = 0
        alpha = float('-inf')
        beta = float('inf')
        max_depth = 3

        result = minimax_ab(depth, index, True, adjusted, alpha, beta, max_depth)
        print(f"Optimal time saved: {result}")
except:
    pass  # Skip if input not provided for this section


=========================================================
=========================================================
=========================================================
=========================================================
			 	SKILL BUILDER : WEEK 4
=========================================================
=========================================================
Single File Programming Question
Problem Statement

Jamie needs to decide what to do for lunch based on their location and whether their lunch is prepared. The decision-making process follows these rules:

If Jamie is at home, they can have lunch at home.

If Jamie is at school and their lunch is prepared, they should bring their lunch.

If Jamie is at school and their lunch is not prepared, they should buy lunch at school.

For any other location, Jamie should check their location and lunch status.

Input format :
The first line contains a string indicating Jamie's location ("home" or "school").

The second line contains a string indicating whether lunch is prepared ("yes" or "no").

Output format :
If Jamie is at home, the output displays "You can have lunch there."

If Jamie is at school and lunch is prepared: the output displays "Bring your lunch."

If Jamie is at school and lunch is not prepared: the output displays "Buy lunch at school."

For any other location: the output displays "Check your location and lunch status."



Refer to the sample output for formatting specifications.

Sample test cases :
Input 1 :
home
no
Output 1 :
You can have lunch at home.
Input 2 :
school
yes
Output 2 :
Bring your lunch.
Input 3 :
school
no
Output 3 :
Buy lunch at school.
Input 4 :
office
yes
Output 4 :
Check your location and lunch status.
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.=========================================================

location = input().strip()
lunch_prepared = input().strip()

if location == "home":
    print("You can have lunch at home.")
elif location == "school" and lunch_prepared == "yes":
    print("Bring your lunch.")
elif location == "school" and lunch_prepared == "no":
    print("Buy lunch at school.")
else:
    print("Check your location and lunch status.")


=========================================================

2. Single File Programming Question
Problem Statement



James is deciding whether to take an umbrella based on the weather conditions for the day. The decision depends on two factors:



Whether it is raining.

The temperature outside.

James needs to determine if they should take an umbrella, based on the following rules:



If it is raining and the temperature is below 30°C, James should take an umbrella.

If it is not raining and the temperature is 20°C or above, James does not need an umbrella.

For any other combination, James should check the weather again.

Input format :
The first line contains a string indicating whether it is raining or not ("yes" or "no").

The second line contains an integer representing the temperature in Celsius.

Output format :
If it is raining and the temperature is below 30°C: Take an umbrella.

If it is not raining and the temperature is 20°C or above: No need for an umbrella.

For any other condition: Check the weather again.



Refer to the sample output for formatting specifications.

Sample test cases :
Input 1 :
yes
25
Output 1 :
Take an umbrella.
Input 2 :
no
22
Output 2 :
No need for an umbrella.
Input 3 :
yes
32
Output 3 :
Check the weather again.
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.


is_raining = input().strip()
temperature = int(input().strip())

if is_raining == "yes" and temperature < 30:
    print("Take an umbrella.")
elif is_raining == "no" and temperature >= 20:
    print("No need for an umbrella.")
else:
    print("Check the weather again.")

=========================================================
=========================================================
3. 
Single File Programming Question
Problem Statement



Alex is working on a simple knowledge base application to manage information about countries and their capitals. The application allows users to query the capital of a country, add new country-capital pairs, or exit the program.



Help Alex to complete this task.

Input format :
The first line consists of an integer (1, 2, or 3).

If the integer is 1, the second line consists of the name of the country (e.g., Japan).

If the integer is 2, the second line consists of the name of the country (e.g., Canada), and the third line consists of the capital of the country (e.g., Ottawa).

If the integer is 3, no additional input is required.

Output format :
For Option 1 (Query Capital):



If the country is in the knowledge base: Output the capital of the country.

If the country is not in the knowledge base: Output Unknown.

For Option 2 (Add Country-Capital Pair):



Output: Added successfully: [Country Name] - [Capital Name]

For Option 3 (Exit Program):



No output. The program terminates.



Refer to the sample output for formatting specifications.

Code constraints :
The user should provide valid options for the menu.

Capital names should be properly formatted.

Sample test cases :
Input 1 :
1
Japan
3
Output 1 :
Tokyo
Input 2 :
2
India
Delhi
3
Output 2 :
Added successfully: India - Delhi
Input 3 :
1
France
2
Italy
Rome
3
Output 3 :
Paris
Added successfully: Italy - Rome
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.
Marks : 10
Negative Marks : 0
image
Fill your code here
Python (3.8)
theme
instruction

knowledge_base = {
    "Japan": "Tokyo",
    "France": "Paris",
    "Germany": "Berlin"
}

while True:
    try:
        choice = int(input().strip())
        if choice == 1:
            country = input().strip()
            print(knowledge_base.get(country, "Unknown"))
        elif choice == 2:
            country = input().strip()
            capital = input().strip()
            knowledge_base[country] = capital
            print(f"Added successfully: {country} - {capital}")
        elif choice == 3:
            break
    except EOFError:
        break


=========================================================
=========================================================
				Challenge Yourself: WEEK 4
=========================================================
=========================================================
Single File Programming Question
Problem Statement



Jamie is managing a small café and needs to handle various operations related to the menu. Depending on the user's choice, Jamie can:

Query the price of a food item.
Add a new food item with its price to the menu.
Update the price of an existing food item.
Remove a food item from the menu.
Input format :
The first line contains an integer representing the action to be performed:

1 for querying the price of a food item.

2 for adding a new food item and its price.

3 for updating the price of an existing food item.

4 for removing a food item from the menu.

If the action is 1, the second line contains the name of the food item.

If the action is 2, the second line contains the name of the food item, and the third line contains the price (as a float).

If the action is 3, the second line contains the name of the food item, and the third line contains the new price (as a float).

If the action is 4, the second line contains the name of the food item.

Output format :
For action 1: Output the price of the food item or Unknown Item if not found.

For action 2: Output confirmation of adding the item with its price or a message if it already exists.

For action 3: Output confirmation of updating the price or a message if the item is not found.

For action 4: Output confirmation of removing the item or a message if the item is not found.

For any invalid action: Output Invalid action.



Refer to the sample output for formatting specifications.

Sample test cases :
Input 1 :
1
Pizza
Output 1 :
10.99
Input 2 :
2
Sandwich
7.49
Output 2 :
Added 'Sandwich' with price $7.49 to the menu.
Input 3 :
3
Pasta
9.49
Output 3 :
Updated 'Pasta' price to $9.49.
Input 4 :
4
Soda
Output 4 :
Removed 'Soda' from the menu.
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

menu = {
    "Pizza": 10.99,
    "Pasta": 8.99,
    "Soda": 1.99
}

try:
    action = int(input().strip())

    if action == 1:
        item = input().strip()
        if item in menu:
            print(f"{menu[item]}")
        else:
            print("Unknown Item")
    elif action == 2:
        item = input().strip()
        price = float(input().strip())
        if item in menu:
            print(f"'{item}' already exists in the menu.")
        else:
            menu[item] = price
            print(f"Added '{item}' with price ${price:.2f} to the menu.")
    elif action == 3:
        item = input().strip()
        price = float(input().strip())
        if item in menu:
            menu[item] = price
            print(f"Updated '{item}' price to ${price:.2f}.")
        else:
            print(f"Item '{item}' not found in the menu.")
    elif action == 4:
        item = input().strip()
        if item in menu:
            del menu[item]
            print(f"Removed '{item}' from the menu.")
        else:
            print(f"Item '{item}' not found in the menu.")
    else:
        print("Invalid action.")
except:
    print("Invalid action.")



=========================================================
=========================================================
Single File Programming Question
Problem Statement



Sophie is trying to decide on an activity based on the time of day, the weather, and her energy level. Depending on these three factors, Sophie needs a suggestion on what activity to do. The decision-making rules are as follows:



Morning:

If it's sunny and Sophie has high energy, she should go for a run.

If it's sunny but she has low energy, she should take a morning walk.

If it's rainy and she has high energy, she should do indoor yoga.

If it's rainy and she has low energy, she should have a cosy breakfast indoors.



Afternoon:

If it's sunny and Sophie has high energy, she should play outdoor sports.

If it's sunny but she has low energy, she should relax at a park.

If it's rainy and she has high energy, she should do a workout at home.

If it's rainy and she has low energy, she should read a book indoors.



Evening:

If it's sunny and Sophie has high energy, she should go for a hike.

If it's sunny but she has low energy, she should watch a sunset.

If it's rainy and she has high energy, she should do an indoor workout.

If it's rainy and she has low energy, she should relax with a movie indoors.

If the time of day is invalid, Sophie will receive the message: Invalid time of day.

Input format :
The first line contains a string representing the time of day ("Morning", "Afternoon", or "Evening").

The second line contains a string representing the weather ("Sunny" or "Rainy").

The third line contains a string representing the energy level ("High" or "Low").

Output format :
The output displays a string suggesting an activity based on Sophie's time of day, weather, and energy level.



Refer to the sample output for formatting specifications.

Sample test cases :
Input 1 :
Morning
Sunny
High
Output 1 :
Go for a run
Input 2 :
Evening
Rainy
Low
Output 2 :
Relax with a movie indoors
Input 3 :
Afternoon
Sunny
Low
Output 3 :
Relax at a park
Input 4 :
Night
Sunny
High
Output 4 :
Invalid time of day.
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.


time_of_day = input().strip()
weather = input().strip()
energy = input().strip()

activities = {
    "Morning": {
        "Sunny": {
            "High": "Go for a run",
            "Low": "Take a morning walk"
        },
        "Rainy": {
            "High": "Do indoor yoga",
            "Low": "Have a cosy breakfast indoors"
        }
    },
    "Afternoon": {
        "Sunny": {
            "High": "Play outdoor sports",
            "Low": "Relax at a park"
        },
        "Rainy": {
            "High": "Do a workout at home",
            "Low": "Read a book indoors"
        }
    },
    "Evening": {
        "Sunny": {
            "High": "Go for a hike",
            "Low": "Watch a sunset"
        },
        "Rainy": {
            "High": "Do an indoor workout",
            "Low": "Relax with a movie indoors"
        }
    }
}

if time_of_day in activities:
    print(activities[time_of_day][weather][energy])
else:
    print("Invalid time of day.")

=========================================================
=========================================================

=========================================================
=========================================================
			 	SKILL BUILDER : WEEK 5
=========================================================
=========================================================
1.Problem Statement
Nandhu is a data scientist working with a dataset named "passenger.csv" which contains information about passengers.
The dataset has missing values that need to be addressed, and Nandhu is also tasked with performing data standardization and normalization for the "Price" column.
Check for Missing Data:
Load the dataset from the provided CSV file and identify the count of missing values in each column.
Handling Missing Data:
Create df_dropna by removing rows with missing values.
Fill missing values in numeric columns using mean (df_mean_filled).
Fill missing values in numeric columns using median (df_median_filled).
Data Standardization and Normalization for "Price" Column:
Use StandardScaler and MinMaxScaler to standardize and normalize the "Price" column, creating df_standardized_price and df_normalized_price datasets, respectively.
Input Format 
The input consists of a CSV file named "passenger.csv," containing information about passengers.
The dataset includes both numeric and non-numeric columns.
Output Format 
The output displays the following information:
Display the count of missing values for each column in the original dataset.
Display the new dataset (df_dropna) after dropping rows with missing values.
Display the standardized values for the "Price" column using StandardScaler.
Display the normalized values for the "Price" column using MinMaxScaler.
Refer to the sample output for the formatting specifications.
Constraints 
The file is located in the same directory as the Python script. 
Nandhu is required to use the SimpleImputer from scikit-learn to handle missing values.
Nandhu should perform both data standardization and normalization for the "Price" column using StandardScaler and MinMaxScaler, respectively
SOLUTION:
main.py
import os
import sys
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer
# Load the dataset
file_path = os.path.join(sys.path[0], "passenger.csv")
df = pd.read_csv(file_path)

# Check for missing data
missing_data = df.isnull().sum()
print("Missing Data:")
print(missing_data)

# Extract numeric columns
numeric_cols = df.select_dtypes(include=['number']).columns
non_numeric_cols = list(set(df.columns) - set(numeric_cols))

# Handling missing data
# Method 1: Drop rows with missing values
df_dropna = df.dropna()

# Method 2: Fill missing values with mean for numeric columns
imputer = SimpleImputer(strategy='mean')
df_mean_filled = pd.DataFrame(imputer.fit_transform(df[numeric_cols]), columns=numeric_cols)

# Concatenate non-numeric columns with filled numeric columns
df_mean_filled = pd.concat([df[non_numeric_cols], df_mean_filled], axis=1)

# Method 3: Fill missing values with median for numeric columns
imputer_median = SimpleImputer(strategy='median')
df_median_filled = pd.DataFrame(imputer_median.fit_transform(df[numeric_cols]), columns=numeric_cols)

# Concatenate non-numeric columns with filled numeric columns
df_median_filled = pd.concat([df[non_numeric_cols], df_median_filled], axis=1)

# Display datasets after handling missing data
print("\nDataset after dropping missing values:")
print(df_dropna)

# Data Standardization and Normalization for "Price" column
scaler = StandardScaler()
min_max_scaler = MinMaxScaler()

# Standardization
df_standardized_price = pd.DataFrame(scaler.fit_transform(df_mean_filled[["Price"]]), columns=["Price_Standardized"])

# Normalization
df_normalized_price = pd.DataFrame(min_max_scaler.fit_transform(df_mean_filled[["Price"]]), columns=["Price_Normalized"])

# Display standardized and normalized values for "Price" column
print("\nStandardized Price:")
print(df_standardized_price)

print("\nNormalized Price:")
print(df_normalized_price)

=========================================================
=========================================================
2.Problem Statement
Ragul is a data scientist working with a dataset named "country.csv," containing information about different countries. He needs to preprocess the data by encoding the "Country" column using both Label Encoding and One-Hot Encoding techniques.

Checking and Encoding "Country" Column:
Load CSV dataset, check if "Country" column exists.
If exists, label encode using LabelEncoder and create "Country_LabelEncoded."
Perform one-hot encoding and create df_one_hot dataset. Display both encoded datasets.
Input Format 
The input consists of a CSV file named "country.csv," containing information about countries.
The dataset includes various columns, with a specific focus on the "Country" column.
Output Format 
The output displays the following information:

Label Encoding Output:
Display the dataset after label encoding the "Country" column, including the new column "Country_LabelEncoded".

One-Hot Encoding Output:
Display the dataset after one-hot encoding the "Country" column, including the newly added one-hot encoded columns.

Refer to the sample output for the formatting specifications.
Constraints 
The file is located in the same directory as the Python script. 
If the "Country" column exists, Ragul should perform both label encoding and one-hot encoding.
Ragul should use LabelEncoder for label encoding and OneHotEncoder for one-hot encoding from scikit-learn.
main.py
import os
import sys
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
# Load the dataset
file_path = os.path.join(sys.path[0], "country.csv")
df = pd.read_csv(file_path, delimiter = '\t')
# Check if "Country" column exists
if "Country" in df.columns:
    # Label encoding for "Country"
    label_encoder = LabelEncoder()
    df["Country_LabelEncoded"] = label_encoder.fit_transform(df["Country"])
    # Display the dataset after label encoding
    print("Dataset after Label Encoding:")
    print(df)
    # One-hot encoding for "Country"
    one_hot_encoder = OneHotEncoder(drop='first', sparse=False)
    one_hot_encoded = one_hot_encoder.fit_transform(df[["Country"]])
    
    # Get feature names for one-hot encoded columns
    feature_names = one_hot_encoder.get_feature_names(['Country'])
    # Create a DataFrame with one-hot encoded columns
    df_one_hot = pd.concat([df, pd.DataFrame(one_hot_encoded, columns=feature_names)], axis=1)
    # Display the dataset after one-hot encoding
    print("\nDataset after One-Hot Encoding:")
    print(df_one_hot)
country.csv
Country	Age	Salary
India	44	72000
US	34	65000
Japan	46	98000
US	35	45000
Japan	23	34000

=========================================================
=========================================================
3.Problem Statement
Bharath is a meteorologist working with a dataset named "Rainfall.csv," which contains information about rainfall in various locations. He needs to preprocess the data by performing label encoding on the 'Location' column.
Read and Display Original Data:
Read the dataset from the given CSV file.
Display the original data to get an overview.
Label Encoding for 'Location' Column:
Initialize a LabelEncoder for the 'Location' column.
Perform label encoding on the 'Location' column.
Display the dataset after label encoding.
Input Format 
The input consists of a CSV file named "Rainfall.csv," containing information about rainfall in different locations. 
The dataset primarily includes a 'Location' column along with other relevant information.
Output Format 
The output displays the following information:
Original Data:
Display the original dataset showing the initial state of the data.
Data After Label Encoding:
Display the dataset after label encoding, specifically highlighting the transformed 'Location' column.
Refer to the sample output for the formatting specifications.
Constraints 
The dataset is expected to have a 'Location' column for label encoding.
Bharath should use LabelEncoder from scikit-learn for performing label encoding.
main.py
import os
import sys
import pandas as pd
from sklearn.preprocessing import LabelEncoder
# Prefixing the path with os.path.join(sys.path[0])
file_path = os.path.join(sys.path[0], "Rainfall.csv")
# Read the data from the CSV file
data = pd.read_csv(file_path, delimiter = ',')
# Display the original data
print("Original Data:")
print(data)

# Initialize label encoder for 'Location' column
label_encoder_location = LabelEncoder()
# Perform label encoding on 'Location' column
data['Location'] = label_encoder_location.fit_transform(data['Location'])
# Display the data after label encoding
print("\nData after Label Encoding for 'Location' column:")
print(data)

=========================================================
=========================================================
				Challenge Yourself: WEEK 5
=========================================================
=========================================================
1.Problem Statement

Vino is a data analyst working with a dataset named "urban_rural.csv" which contains information about urban and rural areas. The dataset has missing values that need to be addressed.

Vino is also tasked with filling missing values for the "Employment," "Population," and "Income" columns. Additionally, Vino needs to perform data standardization and normalization for the "Income" column.

Handling Missing Data and Standardization:
Load the CSV dataset, drop rows with missing values.
Fill categorical "Employment" with mode, "Population" with mean, and "Income" with median.
Display filled values. Standardize "Income_filled_median" using StandardScaler and normalize using MinMaxScaler.
Input Format 
The input consists of a CSV file named "urban_rural.csv," containing information about urban and rural areas. 
The dataset includes both numeric and categorical columns.
Output Format 
The output displays the following information:

Missing Data Information:
Display the count of missing values for each column in the original dataset.

Datasets After Handling Missing Data:
Display the new dataset (df_dropna) after dropping rows with missing values.
Display the filled values for the "Employment" column with mode.
Display the filled values for the "Population" column with mean.
Display the filled values for the "Income" column with median.

Standardized and Normalized Values for "Income" Column:
Display the standardized values for the "Income_filled_median" column.
Display the normalized values for the "Income_filled_median" column.

Refer to the sample output for the formatting specifications.
Constraints 
The dataset may have missing values in numeric columns ("Population" and "Income") and a categorical column ("Employment").
The input dataset may contain a mix of data types.
Vino is required to use the SimpleImputer from scikit-learn to handle missing values.
Vino should perform both data standardization and normalization for the "Income" column using StandardScaler and MinMaxScaler, respectively.
Solution:
main.py
import os
import sys
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer

# Load the dataset
file_path = os.path.join(sys.path[0], "urban_rural.csv")
df = pd.read_csv(file_path, delimiter=',')

# Check for missing data
missing_data = df.isnull().sum()
print("Missing Data:")
print(missing_data)

# Drop rows with missing values
df_dropna = df.dropna()

# Display the dataset after dropping missing values
print("\nDataset after dropping missing values:")
print(df_dropna)

# Handling missing data for Employment
# Assuming "Employment" is categorical, so we will use mode for filling missing values
imputer_employment = SimpleImputer(strategy='most_frequent')
df["Employment_filled"] = imputer_employment.fit_transform(df[["Employment"]])

# Handling missing data for Population
imputer_population = SimpleImputer(strategy='mean')
df["Population_filled_mean"] = imputer_population.fit_transform(df[["Population"]])

# Handling missing data for Income
imputer_income_median = SimpleImputer(strategy='median')
df["Income_filled_median"] = imputer_income_median.fit_transform(df[["Income"]])

# Display filled values for "Employment" with mode
print("\nFilled values for Employment with mode:")
print(df["Employment_filled"])

# Display filled values for "Population" and "Income" with mean and median
print("\nFilled values for Population with mean:")
print(df["Population_filled_mean"])

print("\nFilled values for Income with median:")
print(df["Income_filled_median"])

# Data Standardization and Normalization for "Income" column
scaler = StandardScaler()
min_max_scaler = MinMaxScaler()

# Standardization
df_standardized_income = pd.DataFrame(scaler.fit_transform(df[["Income_filled_median"]]), columns=["Income_Standardized"])

# Normalization
df_normalized_income = pd.DataFrame(min_max_scaler.fit_transform(df[["Income_filled_median"]]), columns=["Income_Normalized"])

# Display standardized and normalized values for "Income" column
print("\nStandardized Income:")
print(df_standardized_income)

print("\nNormalized Income:")
print(df_normalized_income)


=========================================================
=========================================================
2.Problem Statement

Dev is an HR professional working with employee data stored in a CSV file named "employee_data.csv." The dataset includes information such as gender, remarks, and other details. 

He needs to preprocess the data by performing various encoding techniques, including one-hot encoding for the 'Gender' column, ordinal encoding for the 'Remarks' column, and label encoding for the 'Gender' column.

Data Reading and Encoding:
Read and display the original data from the provided CSV file.
Perform one-hot encoding for the 'Gender' column using OneHotEncoder, creating new 'Female' and 'Male' columns.
Ordinally encode the 'Remarks' column with categories ['Not Bad', 'Average', 'Great'].
Convert 'Remarks' to title case for consistency.
Utilize LabelEncoder for label encoding on the 'Gender' column, storing results in 'Gender_Count.'
Input Format 
The input consists of a CSV file named "employee_data.csv," containing information about employees. 
The dataset includes various columns, with a focus on 'Gender,' 'Remarks,' and other relevant information.
Output Format 
The output displays the following information:

Data after Encoding:
Display the dataset after performing one-hot encoding for the 'Gender' column.
Display the dataset after ordinal encoding for the 'Remarks' column.
Display the dataset after label encoding for the 'Gender' column using label encoding.
Refer to the sample output for the formatting specifications.
Constraints 
The dataset may contain relevant columns such as 'Gender' and 'Remarks.'
Dev should use OneHotEncoder, OrdinalEncoder, and LabelEncoder from scikit-learn for the respective encoding tasks.
Solution:
main.py
import os
import sys
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder

# Prefixing the path with os.path.join(sys.path[0])
file_path = os.path.join(sys.path[0], "employee_data.csv")

# Read the data from the CSV file
data = pd.read_csv(file_path)


# One-hot encoding for 'Gender' column
onehot_encoder = OneHotEncoder(sparse=False)
gender_encoded = onehot_encoder.fit_transform(data[['Gender']])
gender_encoded_df = pd.DataFrame(gender_encoded, columns=['Female', 'Male'])

# Concatenate the one-hot encoded columns with the original data
data = pd.concat([data, gender_encoded_df], axis=1)

# Convert 'Not bad' to 'Not Bad' for ordinal encoding
data['Remarks'] = data['Remarks'].str.title()

# Ordinal encoding for 'Remarks' column
ordinal_encoder = OrdinalEncoder(categories=[['Not Bad', 'Average', 'Great']])
data['Remarks'] = ordinal_encoder.fit_transform(data[['Remarks']])

# Count encoding for 'Gender' column
count_encoder = LabelEncoder()
data['Gender_Count'] = count_encoder.fit_transform(data['Gender'])

# Display the data after encoding
print("Data after Encoding:")
print(data)

=========================================================
=========================================================
=========================================================
=========================================================
			 	SKILL BUILDER : WEEK 6
=========================================================
=========================================================
Multi File Programming Question
Problem Statement



Alex, a data scientist, is tasked with predicting whether an employee will leave the company based on their demographic and job-related attributes. The dataset contains features such as age, gender, department, salary, job satisfaction, and years at the company. The target variable is attrition, indicating whether an employee has left the company (1 for yes, 0 for no).



Write a program that reads the dataset from a CSV file, fits a logistic regression model to predict whether an employee will leave the company based on the provided attributes, and evaluates the model's performance using accuracy, F1 score, and a confusion matrix. The program should output the evaluation metrics rounded to four decimal places.

Input format :
The input consists of the filename of the dataset (e.g., "data.csv").

The input datasets are pre-defined as CSV files.

Output format :
The first line of output prints "Accuracy: " followed by the accuracy of the logistic regression model, rounded to four decimal places.

The second line of output prints "F1 Score: " followed by the F1 score of the logistic regression model, rounded to four decimal places.

The third line of output prints "Confusion Matrix: " followed by the confusion matrix of the predictions, printed as a matrix where each row represents the true classes, and each column represents the predicted classes.

The fourth line of output prints "Classification Report:" followed by a detailed classification report that includes precision, recall, F1-score, and support for each class.

The classification report will show:



The precision for each class,
The recall for each class,
The F1-score for each class,
The support for each class,
Overall accuracy, macro average, and weighted average.


Refer to the sample output for the formatting specifications.

Sample test cases :
Input 1 :
data4.csv
Output 1 :
Accuracy: 0.7143
F1 Score: 0.7500
Confusion Matrix:
 [[2 2]
 [0 3]]
Classification Report:
               precision    recall  f1-score   support

           0     1.0000    0.5000    0.6667         4
           1     0.6000    1.0000    0.7500         3

    accuracy                         0.7143         7
   macro avg     0.8000    0.7500    0.7083         7
weighted avg     0.8286    0.7143    0.7024         7

Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

# Read input file name
filename = input().strip()

# Load data
data = pd.read_csv(filename)

# Features and target
X = data.drop('attrition', axis=1)
y = data['attrition']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Train model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluation
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
cr = classification_report(y_test, y_pred, digits=4)

# Output
print(f"Accuracy: {acc:.4f}")
print(f"F1 Score: {f1:.4f}")
print("Confusion Matrix:")
print(cm)
print("Classification Report:")
print(cr)


=========================================================
=========================================================
Multi File Programming Question
Problem Statement



Alex is using the Pima Indians Diabetes dataset to explore the relationship between age and glucose levels through linear regression. 



Write a program that prompts the user for the filename, loads the dataset, and builds a linear regression model to predict 'Glucose' based on 'Age'. Split the data into training and testing sets, train the model, and evaluate its performance using Mean Squared Error (MSE) and R² Score. Finally, display the model's coefficient and intercept, as well as a comparison of actual and predicted glucose values.

Input format :
The input consists of a single line containing the filename of the CSV file diabetes.csv.

Output format :
The output prints the following:

A section displaying the evaluation metrics of the linear regression model
A comparison table of actual vs. predicted glucose values


Refer to the sample output for the formatting specifications.

Sample test cases :
Input 1 :
diabetes.csv
Output 1 :
Linear Regression Model Evaluation:
Mean Squared Error: 1271.096892
R^2 Score: -0.046782
Coefficient: 1.500329
Intercept: 65.282574

Comparison of Actual vs Predicted values:
    Actual   Predicted
0      101   98.289819
1      176  152.301674
2      100  107.291795
3      180  102.790807
4      159  125.295747
5      111  149.301016
6      196  126.796076
7      113   98.289819
8      110  110.292453
9      148  140.299040
10     103  114.793441
11     109  155.302333
12     129   99.790148
13      92  107.291795
14      80   96.789489
15     137  114.793441
16      62  126.796076
17      95  105.791465
18     139  150.801345
19     158  107.291795
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Read input file name
filename = input().strip()

# Load dataset
data = pd.read_csv(filename)

# Define features and target
X = data[['Age']]
y = data['Glucose']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
coef = model.coef_[0]
intercept = model.intercept_

# Output
print("Linear Regression Model Evaluation:")
print(f"Mean Squared Error: {mse:.6f}")
print(f"R^2 Score: {r2:.6f}")
print(f"Coefficient: {coef:.6f}")
print(f"Intercept: {intercept:.6f}")
print()
print("Comparison of Actual vs Predicted values:")
print("    Actual   Predicted")
for i in range(min(20, len(y_test))):
    print(f"{i:>2}  {y_test.iloc[i]:>8}  {y_pred[i]:>11.6f}")



=========================================================
=========================================================

Multi File Programming Question
Problem Statement



Alex, a marketing specialist, is tasked with creating a system to automatically classify emails as spam or non-spam (ham) based on their content and metadata. The dataset consists of several features such as content_length, contains_link, contains_attachment, sender_domain, and spam, where spam is a binary variable indicating whether an email is spam (1) or not (0). Alex needs to build a logistic regression model to predict whether an email is spam based on the available features.



Write a program that loads the dataset from a CSV file, trains a logistic regression model, and predicts the probability of a new email being spam. The program should output the predicted probability, rounded to four decimal places.

Input format :
The input consists of the filename of the dataset (e.g., "data.csv").

The input datasets are pre-defined as CSV files.

Output format :
The output prints "Predicted probability of the new email being spam: followed by the predicted probability of the new email being spam, rounded to four decimal places.



Refer to the sample output for the formatting specifications.

Sample test cases :
Input 1 :
data.csv
Output 1 :
Predicted probability of the new email being spam: 0.7608
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

import pandas as pd
from sklearn.linear_model import LogisticRegression

# Read input file name
filename = input().strip()

# Load dataset
data = pd.read_csv(filename)

# Separate features and target
X = data.drop('spam', axis=1)
y = data['spam']

# Train model
model = LogisticRegression(max_iter=1000)
model.fit(X, y)

# Predict probability for the last row (assumed to be the new email)
new_email = X.tail(1)
proba = model.predict_proba(new_email)[0][1]

# Output
print(f"Predicted probability of the new email being spam: {proba:.4f}")

=========================================================
=========================================================
				Challenge Yourself: WEEK 6
=========================================================
=========================================================
Multi File Programming Question
Problem Statement



Clara, a data analyst, is analyzing a dataset containing health records of patients to predict heart disease. The dataset consists of various attributes such as age, sex, blood pressure, cholesterol levels, and other health indicators. The target variable Target, which is binary indicating the presence (1) or absence (0) of heart disease.



Write a program that reads the dataset from a CSV file, fits a logistic regression model to predict whether a patient has heart disease based on the given attributes, and evaluates the model's performance using accuracy, precision, recall, F1 score, AUC-ROC score, confusion matrix, and specificity. The program should output the evaluation metrics rounded to four decimal places.

Input format :
The input consists of the filename of the dataset (e.g., "data.csv").

The input datasets are pre-defined as CSV files.

Output format :
The first line of output prints "Accuracy Score: " followed by the accuracy of the logistic regression model, rounded to four decimal places.

The second line of output prints "Precision Score: " followed by the precision of the logistic regression model, rounded to four decimal places.

The third line of output prints "Recall Score: " followed by the recall of the logistic regression model, rounded to four decimal places.

The fourth line of output prints "F1 Score: " followed by the F1 score of the logistic regression model, rounded to four decimal places.

The fifth line of output prints "AUC-ROC Score: " followed by the AUC-ROC score of the logistic regression model, rounded to four decimal places.

The sixth line of output prints "Confusion Matrix:" followed by the confusion matrix of the predictions, printed as a matrix where each row represents the true classes, and each column represents the predicted classes.

The last line of output prints "Specificity (True Negative Rate): " followed by the specificity of the logistic regression model, rounded to four decimal places.



Refer to the sample output for the formatting specifications.

Sample test cases :
Input 1 :
data3.csv
Output 1 :
Accuracy Score: 0.9231
Precision Score: 1.0000
Recall Score: 0.8750
F1 Score: 0.9333
AUC-ROC Score: 1.0000
Confusion Matrix:
[[5 0]
 [1 7]]
Specificity (True Negative Rate): 1.0000
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, confusion_matrix)

filename = input()
df = pd.read_csv(filename)

X = df.drop('Target', axis=1)
y = df['Target']

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

print(f"Accuracy Score: {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision Score: {precision_score(y_test, y_pred):.4f}")
print(f"Recall Score: {recall_score(y_test, y_pred):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.4f}")
print(f"AUC-ROC Score: {roc_auc_score(y_test, y_prob):.4f}")
print("Confusion Matrix:")
print(cm)
print(f"Specificity (True Negative Rate): {tn / (tn + fp):.4f}")


=========================================================
=========================================================
Multi File Programming Question
Problem Statement



Hema, a data analyst, needs assistance with a project predicting students' scores based on study hours. A CSV dataset containing 'hours' and 'score' columns, which denote the hours students devoted to studying, and 'score', indicative of their corresponding exam scores. She aims to perform linear regression analysis.



Your task is to prompt for the dataset filename, conduct a regression analysis, and display coefficients, intercept, and R-squared.

Input format :
The input consists of the filename of the dataset (e.g., "data.csv").



The input datasets are pre-defined as CSV files.

Output format :
The output consists of three lines:

"Intercept: " followed by the intercept value of the linear regression model, rounded to four decimal places.

"Coefficient: " followed by the coefficient value of the linear regression model, rounded to four decimal places.

"R-squared: " followed by the R-squared value of the linear regression model, rounded to four decimal places.



Refer to the sample output for the formatting specifications.

Code constraints :
1 ≤ hours ≤ 100

1 ≤ score ≤ 250

Sample test cases :
Input 1 :
data.csv
Output 1 :
Intercept: 65.3340
Coefficient: 1.9824
R-squared: 0.8310
Note :
The program will be evaluated only after the “Submit Code” is clicked.
Extra spaces and new line characters in the program output will result in the failure of the test case.

import pandas as pd
from sklearn.linear_model import LinearRegression

filename = input()
df = pd.read_csv(filename)

X = df[['hours']]
y = df['score']

model = LinearRegression()
model.fit(X, y)

intercept = model.intercept_
coefficient = model.coef_[0]
r_squared = model.score(X, y)

print(f"Intercept: {intercept:.4f}")
print(f"Coefficient: {coefficient:.4f}")
print(f"R-squared: {r_squared:.4f}")

=========================================================
=========================================================

=========================================================
=========================================================
				Challenge Yourself: WEEK 7
=========================================================
=========================================================

1.Problem Statement
Ravi is working with a dataset containing information about various products, including their unit price, units sold, total revenue, and category. He wants to predict the name of a new product based on its characteristics, such as unit price, units sold, total revenue, and category. Ravi plans to use a decision tree classifier for this prediction.

To help Ravi with his task, you're tasked with creating a Python script. The script will assume the dataset is stored in a CSV file named "product.csv". It will read the data from the CSV file, convert categorical features to numerical ones using one-hot encoding, and train a decision tree classifier to predict product names. Then, it will prompt Ravi to input the characteristics of the new product, including its unit price, units sold, total revenue, and category. Based on this input, the script will predict the name of the new product.
Input Format 
The input consists of four lines:
The unit price of the new product (an integer).
The number of units sold for the new product (an integer).
The total revenue generated by the new product (an integer).
The category of the new product (a string).
Output Format 
The output consists of a single line in the following format: "Predicted product name for the new product: " followed by the predicted product name (a string).

Refer to the sample output for the formatting specifications.
Constraints 
The unit price, number of units sold, and total revenue are integers.
The category of the new product is a string with at most 100 characters.
The output predicted product name is a string.
SOLUTION:
main.py
import os
import sys
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# File name
file_name = "product.csv"

# Prefixing the path
file_path = os.path.join(sys.path[0], file_name)

# Read the data from the CSV file
data = pd.read_csv(file_path)

# Convert categorical features to numerical using one-hot encoding
data = pd.get_dummies(data, columns=['Category'])

# Split features and target variable
X = data.drop(columns=['Product_ID', 'Product_Name']).values
y = data['Product_Name'].values

# Create decision tree classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X, y)

# Get input from user for the features of the new product
unit_price = int(input())
units_sold = int(input())
total_revenue = int(input())
category = input()

# Find the corresponding column for the category
category_columns = [col for col in data.columns if col.startswith('Category_')]
category_column = next((col for col in category_columns if col.endswith(category)), None)

# If the category is not found, print an error message and exit
if category_column is None:
    print("Category not found in the dataset.")
    sys.exit()

# Create a new data point for prediction
new_product = [0] * len(X[0])  # Create a list of zeros with the same length as the features
new_product[0] = unit_price
new_product[1] = units_sold
new_product[2] = total_revenue

# Set the corresponding category column to 1
category_index = category_columns.index(category_column)
new_product[3 + category_index] = 1

# Predict the product name for the new product
predicted_product_name = clf.predict([new_product])

print(f"Predicted product name for the new product: {predicted_product_name[0]}")



=========================================================
=========================================================

2. Problem Statement
Reka is creating a program to categorize words into groups like fruit, animal, instrument, or vehicle. Using a neural network with backpropagation, the program learns from a CSV file containing words and their corresponding labels. 
Reka inputs a word, and the program predicts its category based on the trained data. If the word isn't recognized, the program alerts Reka. She plans to refine the program's accuracy through parameter adjustments and more extensive training data.
Input Format 
The input consists of a single word from the predefined CSV file.
Output Format 
For each input word, output a single line in the following format:
The label for '{input_word}' is '{predicted_label}'.
Constraints 
The input word consists of lowercase English alphabets only.
Each word in the input is guaranteed to be present in the provided dictionary.
Each word in the dictionary has a label assigned to it, which can be one of the following categories: Fruit, Animal, Instrument, and Vehicle.
SOLUTION:
main.py
import os
import sys
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# File name
file_name = "product.csv"

# Prefixing the path
file_path = os.path.join(sys.path[0], file_name)

# Read the data from the CSV file
data = pd.read_csv(file_path)

# Convert categorical features to numerical using one-hot encoding
data = pd.get_dummies(data, columns=['Category'])

# Split features and target variable
X = data.drop(columns=['Product_ID', 'Product_Name']).values
y = data['Product_Name'].values

# Create decision tree classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X, y)

# Get input from user for the features of the new product
unit_price = int(input())
units_sold = int(input())
total_revenue = int(input())
category = input()

# Find the corresponding column for the category
category_columns = [col for col in data.columns if col.startswith('Category_')]
category_column = next((col for col in category_columns if col.endswith(category)), None)

# If the category is not found, print an error message and exit
if category_column is None:
    print("Category not found in the dataset.")
    sys.exit()

# Create a new data point for prediction
new_product = [0] * len(X[0])  # Create a list of zeros with the same length as the features
new_product[0] = unit_price
new_product[1] = units_sold
new_product[2] = total_revenue

# Set the corresponding category column to 1
category_index = category_columns.index(category_column)
new_product[3 + category_index] = 1

# Predict the product name for the new product
predicted_product_name = clf.predict([new_product])

print(f"Predicted product name for the new product: {predicted_product_name[0]}")


=========================================================
=========================================================
Skill Builder:
1.Problem Statement      			WEEEK 7
=========================================================
=========================================================

Skill Builder:
1.Problem Statement

Olivia is developing a program to predict loan eligibility based on applicants' credit scores and incomes. Using a dataset containing applicants' credit scores, incomes, and loan eligibility status, Olivia employs a neural network with backpropagation for this task. After loading the dataset, the program trains the neural network to minimize prediction errors by adjusting weights and biases. 

Once training is complete, the program prompts users to input their credit score followed by their income. Based on this input, the program predicts whether the applicant is eligible for a loan or not. If there's an exact match for the user's credit score and income in the dataset, the program outputs the corresponding loan eligibility status. Otherwise, it defaults to predicting "No" as the loan eligibility status.
Input Format 
The first line contains the credit score of the applicant as an integer.
The second line contains the income of the applicant as an integer.
Output Format 
The output consists of a single line containing either "Eligible for loan: Yes" if the applicant is predicted to be eligible for a loan, or "Eligible for loan: No" if not.

Refer to the sample output for the formatting specifications.
Constraints 
The credit score and income are both integers.
The credit score is between 600 and 900.
The income is between 30000 and 75000.
Solution:
main.py
import os
import sys
import pandas as pd
import numpy as np

# Prefixing the path
file_path = os.path.join(sys.path[0], "loan.csv")

# Load data from the CSV file
data = pd.read_csv(file_path)

# Define the sigmoid function for activation
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Backpropagation algorithm
class NeuralNetwork:
    def __init__(self):
        # Initialize weights and biases
        self.w_score = np.random.rand()
        self.w_income = np.random.rand()
        self.bias = np.random.rand()
        self.learning_rate = 0.1

    def train(self, score, income, eligible_loan):
        # Feedforward
        weighted_sum = self.w_score * score + self.w_income * income + self.bias
        output = sigmoid(weighted_sum)

        # Backpropagation
        target = 1 if eligible_loan == 'Yes' else 0
        error = target - output
        d_w_score = error * score * output * (1 - output)
        d_w_income = error * income * output * (1 - output)
        d_bias = error * output * (1 - output)

        # Update weights and biases
        self.w_score += self.learning_rate * d_w_score
        self.w_income += self.learning_rate * d_w_income
        self.bias += self.learning_rate * d_bias

    def predict(self, score, income):
        # Predict output
        weighted_sum = self.w_score * score + self.w_income * income + self.bias
        return sigmoid(weighted_sum)

# Initialize neural network
model = NeuralNetwork()

# Train the neural network
for _, row in data.iterrows():
    score = row['Score']
    income = row['Income']
    eligible_loan = row['EligibleLoan']
    model.train(score, income, eligible_loan)

# Get user input
user_score = int(input())
user_income = int(input())

# Find the corresponding "EligibleLoan" status or default to "No"
score_income_match = data[(data['Score'] == user_score) & (data['Income'] == user_income)]
if not score_income_match.empty:
    predicted_eligibility = score_income_match.iloc[0]['EligibleLoan']
else:
    predicted_eligibility = "No"

print("Eligible for loan:", predicted_eligibility)
=========================================================
=========================================================
2.Problem Statement

﻿Jenifer needs to predict the species of iris flowers based on their characteristics. She has collected data in a file named "iris.csv" containing measurements of sepal length, sepal width, petal length, and petal width for various iris flowers, along with their corresponding species. and wants to use a decision tree classifier for this task. 

Jenifer will input the characteristics of a new iris flower, and the program will predict its species based on the trained model.
Input Format 
The input consists of four lines, each containing a floating-point number representing a measurement of a new iris flower:
The first line contains the sepal length.
The second line contains the sepal width.
The third line contains the petal length.
The fourth line contains the petal width.
Output Format 
The output is a single line that displays the predicted species for the new flower in the following format:
"Predicted species for the new flower: species_name"

Refer to the sample output for the formatting specifications.
Constraints 
1.0 ≤ Sepal Length ≤ 8.0
0.1 ≤ Sepal Width ≤ 4.0
1.0 ≤ Petal Length ≤ 7.0
0.1 ≤ Petal Width ≤ 2.5
Solution:
main.py
import os
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Load the Iris dataset
iris_path = os.path.join(sys.path[0], "iris.csv")
iris_data = pd.read_csv(iris_path)

X = iris_data.drop(columns=['species']).values
y = iris_data['species'].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a decision tree classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Get input from user for the features of the new flower
sepal_length = float(input())
sepal_width = float(input())
petal_length = float(input())
petal_width = float(input())

new_flower = [[sepal_length, sepal_width, petal_length, petal_width]]

# Predict the species for the new flower
predicted_species = clf.predict(new_flower)

print(f"Predicted species for the new flower: {predicted_species[0]}")

=========================================================
=========================================================
3.Problem Statement

﻿Barsh is building a sentiment prediction program using a neural network with backpropagation. He loads a dataset of statements and their positive or negative sentiments, constructs the neural network, trains it, and prompts user input for prediction. The program then predicts sentiment based on the trained network. 
Input Format 
The input consists of a single statement from the predefined CSV file.
Output Format 
For each input statement, print "Positive" if the sentiment is positive and "Negative" if it's negative.

Refer to the sample output for the formatting specifications.
Constraints 
The statements are case-sensitive.
Solution:
main.py
import os
import sys
import csv
import numpy as np

# Define the backpropagation algorithm
class BackPropagation:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # Initialize weights
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)
        
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_derivative(self, x):
        return x * (1 - x)
    
    def forward(self, inputs):
        self.hidden = self.sigmoid(np.dot(inputs, self.weights_input_hidden))
        self.output = self.sigmoid(np.dot(self.hidden, self.weights_hidden_output))
        return self.output
    
    def backward(self, inputs, target):
        # Calculate error
        output_error = target - self.output
        output_delta = output_error * self.sigmoid_derivative(self.output)
        
        # Backpropagate error to hidden layer
        hidden_error = output_delta.dot(self.weights_hidden_output.T)
        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden)
        
        # Update weights
        self.weights_hidden_output += self.hidden.T.dot(output_delta) * self.learning_rate
        self.weights_input_hidden += inputs.T.dot(hidden_delta) * self.learning_rate

# Load training data from CSV
def load_training_data(filename):
    statements = []
    results = []
    with open(filename, 'r', newline='') as file:
        reader = csv.reader(file)
        next(reader)  # Skip header row
        for row in reader:
            statements.append(row[0])
            results.append(1 if row[1] == 'pos' else 0)  # Convert 'pos' to 1, 'neg' to 0
    return statements, results

def main():
    filename = os.path.join(sys.path[0], "statement.csv")
    statements, results = load_training_data(filename)
    
    # Define input and output size based on the number of unique words in statements
    unique_words = set(word for statement in statements for word in statement.split())
    input_size = len(unique_words)
    output_size = 1
    
    # Define hidden layer size
    hidden_size = 8
    
    # Initialize backpropagation algorithm
    backprop = BackPropagation(input_size, hidden_size, output_size)
    
    # Training loop
    epochs = 1000
    for epoch in range(epochs):
        for statement, result in zip(statements, results):
            # Convert statement to one-hot encoded input
            input_data = np.zeros((1, input_size))
            for i, word in enumerate(unique_words):
                if word in statement:
                    input_data[0, i] = 1
            
            # Forward pass
            output = backprop.forward(input_data)
            
            # Backward pass
            backprop.backward(input_data, result)
    
    # User input for prediction
    user_input = input().strip().lower()
    
    # Convert user input to one-hot encoded input
    input_data = np.zeros((1, input_size))
    for i, word in enumerate(unique_words):
        if word in user_input:
            input_data[0, i] = 1
    
    # Predict sentiment
    prediction = backprop.forward(input_data)[0, 0]
    if prediction > 0.5:
        print("Positive")
    else:
        print("Negative")

if __name__ == "__main__":
    main()


=========================================================
=========================================================
				Challenge Yourself: WEEK 8
=========================================================
=========================================================
1) Problem Statement

Santa needs to develop a model to predict employee attrition based on certain features. He has a CSV dataset containing employee data, including features like 'Age', 'YearsAtCompany', 'MonthlyIncome', 'PerformanceRating', 'WorkLifeBalance', and 'YearsInCurrentRole', along with the target variable 'Attrition'. 

Santa requires a Python program that prompts him for this dataset's filename. The program should load the data, split it into training and testing sets, train a Naive Bayes classifier using Gaussian, make predictions on the testing set, and finally, output the accuracy of the model and the classification report.
Input Format 
The input consists of a single line containing the dataset filename in CSV format.
Output Format 
The output should consist of two parts:

The accuracy of the model is rounded to two decimal places, preceded by the exact string "Accuracy: ".
The classification report, including precision, recall, F1-score, and support for each class, as well as accuracy and support metrics, is formatted as shown in the sample output.

Refer to the sample output for the formatting specifications.
Constraints 
The dataset should contain columns for age, years at the company, monthly income, performance rating, work-life balance, years in the current role, and the attrition label.
The target variable (attrition) should be binary, with 'Yes' and 'No' indicating no attrition.
The precision, recall, and F1-score in the classification report should be between 0 and 1, inclusive.



SOLUTION:
HEADER:
import os
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB  # Updated to GaussianNB
from sklearn.metrics import accuracy_score, classification_report

def main():
    # Prompt the user for the input file name
    file_name = input()

    # Constructing the file path using os and sys
    csv_file = os.path.join(sys.path[0], file_name)

    # Load the dataset
    df = pd.read_csv(csv_file, delimiter=',')

    # Prepare the data
    features = df[['Age', 'YearsAtCompany', 'MonthlyIncome', 'PerformanceRating', 'WorkLifeBalance', 'YearsInCurrentRole']]
    target = df['Attrition']

    # Split the dataset into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)


Main.py
# Train the Naive Bayes classifier (using GaussianNB)
    clf = GaussianNB()
    clf.fit(X_train, y_train)

    # Make predictions
    y_pred = clf.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)

    # Rename 'class' to avoid keyword conflict
    classification_rep = classification_report(y_test, y_pred, zero_division=0)
Footer:
# Print the report
    print("Accuracy:", f"{accuracy:.2f}")
    print("Classification Report:\n", classification_rep)

if __name__ == "__main__":
    main()

=========================================================
=========================================================

2)Problem Statement

Alice is analyzing a dataset containing measurements of iris flowers for species classification. She wants to evaluate the performance of a Support Vector Machine (SVM) classifier trained on the dataset. 

Write a program to load a CSV file containing the dataset, split it into training and testing sets, train an SVM classifier with a linear kernel, and evaluate its performance using various metrics like accuracy, precision, recall, F1-score, confusion matrix, and classification report.
Input Format 
The input consists of a single line containing the name of the CSV file (e.g., iris.csv).
Output Format 
The output prints the accuracy of the SVM classifier (in percentage) with the following format:
"Accuracy of the SVM classifier: <accuracy>%"
Metrics for precision, recall, and F1-score:
"Precision: <precision>"
"Recall: <recall>"
"F1-Score: <f1-score>"
The confusion matrix in the format:
"Confusion Matrix:" followed by
"[[<value1> <value2> <value3>]
[<value4> <value5> <value6>]
[<value7> <value8> <value9>]]"
"The classification report:" 
"<detailedclassificationreport>"



Solution:
main.py
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC  # Support Vector Classifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Prompt the user to enter the CSV filename
file_name = input()

# Get the current directory of the script
current_directory = os.path.dirname(__file__)

# Construct the full path to the CSV file
file_path = os.path.join(current_directory, file_name)

# Load the dataset
iris_data = pd.read_csv(file_path)

# Check the first few rows of the dataset to understand its structure
# print(iris_data.head())  # Uncomment to inspect data

# Assuming the last column contains the labels (species)
X = iris_data.iloc[:, :-1]  # Features: All columns except the last one
y = iris_data.iloc[:, -1]   # Labels: Last column (species)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the SVM classifier
svm_classifier = SVC(kernel='linear')  # You can choose different kernels like 'linear', 'poly', or 'rbf'

# Train the model
svm_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_classifier.predict(X_test)

# Evaluate the model

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the SVM classifier: {accuracy * 100:.2f}%")

# Precision (average for all classes)
precision = precision_score(y_test, y_pred, average='weighted')
print(f"Precision: {precision:.2f}")

# Recall (average for all classes)
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall:.2f}")

# F1-Score (average for all classes)
f1 = f1_score(y_test, y_pred, average='weighted')
print(f"F1-Score: {f1:.2f}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Classification Report (includes precision, recall, f1-score for each class)
class_report = classification_report(y_test, y_pred)
print("Classification Report:")
print(class_report)

=========================================================
=========================================================
Skill Builder:
1.Problem Statement      			WEEEK 8
=========================================================
=========================================================
Skill Builder:
1.Problem Statement

James wants to build a text classification model to predict the category of documents. He has a dataset containing documents labeled with specific categories. He needs a Python program that will prompt him to input the file name containing the dataset. 

The program will load the data, split it into training and testing sets, vectorize the text data, train a Multinomial Naive Bayes classifier, make predictions on the testing set, and finally, evaluate the model's performance in terms of accuracy and classification report.
Input Format 
The input consists of a single line containing the filename of the dataset in CSV in the following format.

'document': Text data representing documents.
'label': Labels associated with each document (Positive, Negative, Neutral)
Output Format 
The output displays the following information:

Accuracy:
The accuracy of the Naive Bayes classifier on the testing set rounded off to 2 decimal points.

Classification Report:
The classification report includes precision, recall, and F1 scores for each class in the testing set.

Refer to the sample output for the formatting specifications.
Constraints 
The dataset is reasonably small and fits into memory.
The 'data.csv' file contains valid data with non-null values in both the 'document' and 'label' columns.
Solution:
main.py
import os
import sys
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Prompt the user for the input file name
file_name = input()

# Use os.path.join to construct the file path, assuming the file is in the same directory as the script
file_path = os.path.join(sys.path[0], file_name)

# Load the data with a comma as the delimiter
data = pd.read_csv(file_path, delimiter=',')

# Separate features and labels
X = data['document']
y = data['label']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize the text data
vectorizer = CountVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Train a Naive Bayes classifier
classifier = MultinomialNB()
classifier.fit(X_train_vectorized, y_train)

# Make predictions
predictions = classifier.predict(X_test_vectorized)

# Evaluate the model
accuracy = round(accuracy_score(y_test, predictions), 2)
report = classification_report(y_test, predictions, zero_division=1)

print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(report)
=========================================================
=========================================================

2.Problem Statement

Janani needs to develop a model for predicting labels based on certain environmental parameters. She has a dataset in CSV format containing columns for 'Temperature', 'Humidity', 'Wind_Speed', 'Pressure', and 'Cloud_Cover', along with corresponding labels. 

Janani requires a Python program that prompts her for the filename containing this dataset. The program should load the data, split it into training and testing sets, train a Gaussian Naive Bayes classifier, make predictions on the testing set, and finally, output the accuracy of the model rounded to two decimal places and the confusion matrix.
Input Format 
The input consists of a single line containing the filename of the dataset in CSV format.
Output Format 
The output should consist of two parts:

The accuracy of the model, rounded to two decimal places, with the exact string "Accuracy: " preceding it.
The confusion matrix, where each row represents the true labels and each column represents the predicted labels, is formatted as shown in the sample output.

Refer to the sample output for the formatting specifications.
Constraints 
The dataset should have columns for temperature, humidity, wind speed, pressure, cloud cover, and the label.
The label should be binary, where 'yes' indicates rain and 'no' indicates no rain.
Solution:
Header:
import os
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix

def main():
    # Prompt the user for the input file name
    file_name = input()
    
    # Construct the file path
    file_path = os.path.join(sys.path[0], file_name)
    
    # Load the dataset
    df = pd.read_csv(file_path, delimiter=',')
    
    # Prepare the data
    X = df[['Temperature', 'Humidity', 'Wind_Speed', 'Pressure', 'Cloud_Cover']]
    y = df['Label']
    
    # Split the dataset into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
main.py
    # Train the Naive Bayes classifier (Gaussian Naive Bayes for continuous features)
    clf = GaussianNB()
    clf.fit(X_train, y_train)
    
    # Make predictions
    y_pred = clf.predict(X_test)
    
    # Calculate accuracy and confusion matrix
    accuracy = accuracy_score(y_test, y_pred)
    confusion_mat = confusion_matrix(y_test, y_pred)
Footer
 # Print results
    print(f"Accuracy: {accuracy:.2f}")
    print("\nConfusion Matrix:\n", confusion_mat)

if __name__ == "__main__":
    main()

=========================================================
=========================================================
3.Problem Statement

In a population where 10% of individuals have a specific medical condition, a patient undergoes two independent diagnostic tests: Test A and Test B. Test A has an 80% true positive rate and a 90% true negative rate, while Test B has a 70% true positive rate and an 80% true negative rate. 

Using Baye's rule, we aim to calculate the probability that the patient has the medical condition given both Test A and Test B yield positive results.
Input Format 
No Console Input
Output Format 
The output displays the probability that the individual has the medical condition given that both Test A and Test B yield positive results (float between 0 and 1, inclusive), rounded to two decimal places.

Refer to the sample output for the formatting specifications.
Constraints 
0 ≤ pcondition, ppositive given condition A, ppositive given condition B, pnegative given no condition A, pnegative given no condition B ≤ 1
The sum of ppositive given condition A and pnegative given no condition A should not exceed 1.
The sum of ppositive given condition B and pnegative given no condition B should not exceed 1.
Solution
# Define the probabilities
p_condition = 0.1  # Probability of a person having the medical condition (prior probability)
p_no_condition = 1 - p_condition  # Probability of a person not having the medical condition

# Conditional probabilities for Test A
p_positive_given_condition_A = 0.8  # Probability of Test A being positive if the person has the condition (true positive rate)
p_negative_given_no_condition_A = 0.9  # Probability of Test A being negative if the person doesn't have the condition (true negative rate)

# Conditional probabilities for Test B
p_positive_given_condition_B = 0.7  # Probability of Test B being positive if the person has the condition (true positive rate)
p_negative_given_no_condition_B = 0.8  # Probability of Test B being negative if the person doesn't have the condition (true negative rate)

# This is the probability of both tests being positive
# It is the sum of the probability of both tests being positive given the person has the condition
# and the probability of both tests being positive given the person doesn't have the condition
p_positive_both_tests = (p_condition * p_positive_given_condition_A * p_positive_given_condition_B) + \
                        (p_no_condition * (1 - p_negative_given_no_condition_A) * (1 - p_negative_given_no_condition_B))

# Apply Bayes' rule to find the probability of the person having the condition given both tests are positive
p_condition_given_positive_both_tests = (p_condition * p_positive_given_condition_A * p_positive_given_condition_B) / p_positive_both_tests

# Print the result
print('{:.2%}'.format(p_condition_given_positive_both_tests))

=========================================================
=========================================================


4.Problem Statement

Eleanor, a data analyst, is working on an email classification project to detect spam emails using a CSV file dataset. The dataset includes features like contains_link, contains_attachment, sender_domain, and spam labels. 

Write a program to preprocess the dataset, train an SVM model to classify emails as spam or not, and evaluate the model using various metrics like accuracy, F1 score, precision, recall, and ROC AUC. The program should prompt the user for the file name, preprocess the data, and output evaluation metrics.
Input Format 
The input consists of the filename of the dataset (e.g., "data.csv").
The input datasets are pre-defined as CSV files.
Output Format 
The output prints the evaluation metrics:
Accuracy
F1 Score
Precision
Recall
ROC AUC
Confusion matrix and classification report.


Solution 1
main.py
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_auc_score, classification_report

# Prompt the user to enter the filename
file_name = input()

# Get the current directory of the script
current_directory = os.path.dirname(__file__)

# Construct the full path to the CSV file
file_path = os.path.join(current_directory, file_name)

# Load the dataset
emails = pd.read_csv(file_path)

# Display the first few rows of the dataset to check its structure
#print(emails.head())

# Preprocess the dataset
def preprocess_data(emails):
    # Encoding categorical features (Yes/No -> 1/0)
    label_encoder = LabelEncoder()
    
    # Encode 'contains_link', 'contains_attachment' (Yes=1, No=0)
    emails['contains_link'] = label_encoder.fit_transform(emails['contains_link'])
    emails['contains_attachment'] = label_encoder.fit_transform(emails['contains_attachment'])
    
    # Encode 'sender_domain' (gmail.com, yahoo.com, hotmail.com -> 0, 1, 2)
    emails['sender_domain'] = label_encoder.fit_transform(emails['sender_domain'])
    
    # Split features (X) and target (y)
    X = emails.drop(columns=['email_id', 'spam'])
    y = emails['spam']
    
    # Split the data into training and testing sets (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Feature scaling - Standardizing the feature values
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train_scaled, X_test_scaled, y_train, y_test

# Train and evaluate the SVM model
def train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test):
    # Train the SVM model with a linear kernel
    svm_model = SVC(kernel='linear', random_state=42)
    svm_model.fit(X_train_scaled, y_train)
    
    # Predict on the test data
    y_pred = svm_model.predict(X_test_scaled)
    
    # Evaluate the model using various metrics
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)
    
    # Output the results
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    print("Confusion Matrix:")
    print(conf_matrix)
    print("Classification Report:")
    print(class_report)

# Main function to handle the full process
def main():
    # Preprocess the data
    X_train_scaled, X_test_scaled, y_train, y_test = preprocess_data(emails)
    
    # Train and evaluate the SVM model
    train_and_evaluate_svm(X_train_scaled, X_test_scaled, y_train, y_test)

if __name__ == "__main__":
    main()


=========================================================
=========================================================
							WEEK 9
						SKILL BUILDER
=========================================================
=========================================================
Q1. Problem Statement
Amir has a dataset named 'data.csv' containing two variables, M1 and M2, along with their corresponding labels. He wants to use Python's scikit-learn library to perform k-means clustering on the data. Amir's goal is to predict the cluster for a new case based on the values of M1 and M2. 
The program reads the data from the CSV file, extracts M1 and M2 values, applies k-means clustering with three centroids, and prompts Amir for new M1 and M2 values. Finally, it predicts the cluster for the new case and displays the result to Amir.
Note: The random_state is set to 42.
Input Format 
The first line contains a floating-point number representing the value of variable M1.
The second line contains a floating-point number representing the value of variable M2.
The predefined input dataset is given as data.csv file. 
Output Format 
The output displays a single line in the following format:
"M1 = {M1_value} and M2 = {M2_value} predict cluster is: {cluster_prediction}".
Constraints 
1.0 ≤ M1, M2 ≤ 6.999
SOLUTION:

import os
import sys
import numpy as np
from sklearn.cluster import KMeans
import pandas as pd

# Define the path to the CSV file
csv_file_path = os.path.join(sys.path[0], 'data.csv')
# Read the data from the CSV file
df = pd.read_csv(csv_file_path)
# Extract the data from the DataFrame
data = df[['M1', 'M2']].values
# Perform k-means clustering with 3 centroids
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(data)
# Get the new case input from the user
var1 = float(input())
var2 = float(input())
new_case = np.array([[var1, var2]])
# Predict the cluster for the new case
prediction = kmeans.predict(new_case)
print(f'M1 = {var1} and M2 = {var2} predict cluster is: {prediction[0]}')

=========================================================
=========================================================

Q2)Problem Statement
Stephen needs to categorize customers based on their sales amount and total sales amount. He has a dataset containing customer IDs, sales amount, category, and total sales amount. He wants to use k-means clustering with two centroids to cluster the customers. 
Stephen needs a simple program that reads the data from a CSV file, performs k-means clustering, and then allows him to input new sales amount and total sales amount values to predict which cluster the new customer belongs to.
Note: The random_state is set to 42.
Input Format 
The first line consists of a floating-point number representing the sales amount (var1).
The second line consists of a floating-point number representing the total sales amount (var2).
The predefined input dataset is given as data.csv file. 
Output Format 
The output displays a sigle line in the following format:
"SALES_AMOUNT: {var1}, TOTAL_SALES_AMOUNT: {var2} - Cluster: {cluster_number}"
where {var1} and {var2} are the input sales amount and total sales amount, respectively, and {cluster_number} is the predicted cluster for the new customer.
Constraints 
100.0 ≤ SALES_AMOUNT, TOTAL_SALES_AMOUNT ≤ 1500.00
SOLUTION:
import os
import sys
import numpy as np
from sklearn.cluster import KMeans
import pandas as pd
# Define the path to the CSV file
csv_file_path = os.path.join(sys.path[0], 'data.csv')
# Read the data from the CSV file
df = pd.read_csv(csv_file_path)

# Perform k-means clustering with 2 centroids using 'SALES_AMOUNT' and 'TOTAL_SALES_AMOUNT'
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(df[['SALES_AMOUNT', 'TOTAL_SALES_AMOUNT']])
# Get the new case input from the user
var1 = float(input())
var2 = float(input())
new_case = np.array([[var1, var2]])
# Predict the cluster for the new case
prediction = kmeans.predict(new_case)
print(f'SALES_AMOUNT: {var1}, TOTAL_SALES_AMOUNT: {var2} - Cluster: {prediction[0]}')

=========================================================
=========================================================

 Q3) PROBLEM STATEMENT
 Jeevan is managing financial data for customers, including CUSTOMER_ID, REVENUE, EXPENSES, and CATEGORY. He is interested in clustering customers based on their revenue and expenses using k-means clustering with two centroids. 
Jeevan needs a program that reads the data from a CSV file, performs k-means clustering, and allows him to input new values for REVENUE and EXPENSES to predict the cluster to which a new customer belongs.
Note: The random_state is set to 42.
Input Format 
The first line consists of a floating-point number representing the revenue (var1) of the new customer.
The second line consists of a floating-point number representing the expenses (var2) of the new customer.
The predefined input dataset is given as data.csv file. 
Output Format 
The output displays a single line in the following format:
"REVENUE: {var1}, EXPENSES: {var2} - Cluster: {cluster_number}"
where {var1} and {var2} are the input revenue and expenses, respectively, and {cluster_number} is the predicted cluster for the new customer.
Constraints 
100.0 ≤ REVENUE, EXPENSES ≤ 300.00
SOLUTION:
import os
import sys
import numpy as np
from sklearn.cluster import KMeans
import pandas as pd
# Define the path to the CSV file
csv_file_path = os.path.join(sys.path[0], 'data.csv')
# Read the data from the CSV file
df = pd.read_csv(csv_file_path)
# Perform k-means clustering with 2 centroids
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(df[['REVENUE', 'EXPENSES']])
# Get the new case input from the user
var1 = float(input())
var2 = float(input())
new_case = np.array([[var1, var2]])
# Predict the cluster for the new case
prediction = kmeans.predict(new_case)
print(f'REVENUE: {var1}, EXPENSES: {var2} - Cluster: {prediction[0]}')
=========================================================
=========================================================
						WEEK-9
					CHALLENGE YOURSELF
=========================================================
=========================================================
Q1) Problem Statement
Diya is working with a dataset named 'data.csv,' which includes two columns, COLUMN_A and COLUMN_B, along with a corresponding CATEGORY. She aims to perform k-means clustering on the data using Python's scikit-learn library. 
The program reads the data from the CSV file, applies k-means clustering with four centroids, and prompts Diya for new COLUMN_A and COLUMN_B values. Subsequently, it predicts the cluster for the new case and displays the result, associating it with the corresponding variables.
Note: The random_state is set to 42.
Input Format 
The first line consists of a floating-point number representing the value of COLUMN_A.
The second line consists of a floating-point number representing the value of COLUMN_B.
The predefined input dataset is given as data.csv file.
Output Format 
The output displays a single line in one of the following format:
"COLUMN_A: {value_of_COLUMN_A} and COLUMN_B: {value_of_COLUMN_B} - {cluster_prediction}"
Constraints 
0.0 ≤ COLUMN_A, COLUMN_B ≤ 1.999
SOLUTION
import os
import sys
import numpy as np
from sklearn.cluster import KMeans
import pandas as pd
# Define the path to the CSV file
csv_file_path = os.path.join(sys.path[0], 'data.csv')
# Read the data from the CSV file
df = pd.read_csv(csv_file_path)
# Perform k-means clustering with 3 centroids
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(df[['COLUMN_A', 'COLUMN_B']])
# Get the new case input from the user
var1 = float(input())
var2 = float(input())
new_case = np.array([[var1, var2]])
# Predict the cluster for the new case
prediction = kmeans.predict(new_case)
print(f'COLUMN_A: {var1} and COLUMN_B: {var2} - {prediction[0]}')

=========================================================
=========================================================

Q2) Problem Statement
Jansi has a dataset with customer information, including ID, VALUE_A, VALUE_B, and CATEGORY. She is interested in categorizing customers based on their values for VALUE_A and VALUE_B using k-means clustering with two centroids. 
Jansi wants a program that reads the data from a CSV file, performs k-means clustering, and allows her to input new values for VALUE_A and VALUE_B to predict the cluster to which a new customer belongs.
Note: The random_state is set to 42.
Input Format 
The first line consists of a floating-point number representing the value for VALUE_A of the new customer.
The second line consists of a floating-point number representing the value for VALUE_B of the new customer.
The predefined input dataset is given as data.csv file. 
Output Format 
The output displays a single line in one of the following format:
"VALUE_A: {var1}, VALUE_B: {var2} - Cluster: {cluster_number}"
where {var1} and {var2} are the input values for VALUE_A and VALUE_B, respectively, and {cluster_number} is the predicted cluster for the new customer.
Constraints 
0.0 ≤ VALUE_A, VALUE_B ≤ 100.00
SOLUTION
import os
import sys
import numpy as np
from sklearn.cluster import KMeans
import pandas as pd
# Define the path to the CSV file
csv_file_path = os.path.join(sys.path[0], 'data.csv')
# Read the data from the CSV file
df = pd.read_csv(csv_file_path)
# Perform k-means clustering with 2 centroids
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(df[['VALUE_A', 'VALUE_B']])
# Get the new case input from the user
var1 = float(input())
var2 = float(input())
new_case = np.array([[var1, var2]])
# Predict the cluster for the new case
prediction = kmeans.predict(new_case)
print(f'VALUE_A: {var1}, VALUE_B: {var2} - Cluster: {prediction[0]}')



